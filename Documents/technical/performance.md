# æ€§èƒ½ä¼˜åŒ–æŒ‡å—

> **é˜…è¯»æ—¶é—´**: 18åˆ†é’Ÿ  
> **éš¾åº¦**: â­â­â­â­  
> **å‰ç½®çŸ¥è¯†**: Pythonæ€§èƒ½ä¼˜åŒ–ã€ç¼“å­˜åŸç†ã€å¼‚æ­¥ç¼–ç¨‹

---

## ğŸ“– ç›®å½•

- [æ¦‚è¿°](#æ¦‚è¿°)
- [æ€§èƒ½åŸºå‡†](#æ€§èƒ½åŸºå‡†)
- [ç¼“å­˜ç­–ç•¥](#ç¼“å­˜ç­–ç•¥)
- [æ‰¹é‡å¤„ç†](#æ‰¹é‡å¤„ç†)
- [å¼‚æ­¥ä¼˜åŒ–](#å¼‚æ­¥ä¼˜åŒ–)
- [æ•°æ®åº“ä¼˜åŒ–](#æ•°æ®åº“ä¼˜åŒ–)
- [å‰ç«¯ä¼˜åŒ–](#å‰ç«¯ä¼˜åŒ–)
- [ç›‘æ§å’Œåˆ†æ](#ç›‘æ§å’Œåˆ†æ)
- [æœ€ä½³å®è·µ](#æœ€ä½³å®è·µ)

---

## æ¦‚è¿°

### æ€§èƒ½ç›®æ ‡

| æ“ä½œ | ç›®æ ‡å“åº”æ—¶é—´ | å½“å‰æ€§èƒ½ | ä¼˜åŒ–çŠ¶æ€ |
|------|-------------|---------|---------|
| é¡µé¢åŠ è½½ | < 2s | 1.5s | âœ… è¾¾æ ‡ |
| æœç´¢æŸ¥è¯¢ | < 500ms | 300ms | âœ… è¾¾æ ‡ |
| å›¾è°±æ¸²æŸ“ | < 3s | 2s | âœ… è¾¾æ ‡ |
| æ–‡æ¡£ä¸Šä¼  | < 5s | 3s | âœ… è¾¾æ ‡ |
| æ™ºèƒ½é—®ç­” | < 3s | 4s | âš ï¸ éœ€ä¼˜åŒ– |
| å›¾è°±æ„å»º | < 10s/æ–‡æ¡£ | 8s | âœ… è¾¾æ ‡ |

### æ€§èƒ½ç“¶é¢ˆåˆ†æ

```
1. æ™ºèƒ½é—®ç­” (4ç§’)
   â”œâ”€ RAGFlowæ£€ç´¢: 1s
   â”œâ”€ Qwenç”Ÿæˆ: 2.5s  âš ï¸ ä¸»è¦ç“¶é¢ˆ
   â””â”€ ç»“æœæ¸²æŸ“: 0.5s

2. å›¾è°±æ„å»º (8ç§’)
   â”œâ”€ è·å–æ–‡æ¡£chunks: 1s
   â”œâ”€ Qwenå®ä½“æŠ½å–: 5s  âš ï¸ ä¸»è¦ç“¶é¢ˆ
   â””â”€ æ•°æ®åº“å†™å…¥: 2s

3. å›¾è°±æ¸²æŸ“ (2ç§’)
   â”œâ”€ æ•°æ®åº“æŸ¥è¯¢: 0.5s
   â”œâ”€ Pyvisæ„å»º: 1s
   â””â”€ HTMLæ¸²æŸ“: 0.5s
```

---

## æ€§èƒ½åŸºå‡†

### æµ‹è¯•ç¯å¢ƒ

```
ç¡¬ä»¶ï¼š
- CPU: Intel i7-12700K (12æ ¸)
- RAM: 32GB DDR4
- SSD: 1TB NVMe

è½¯ä»¶ï¼š
- Python 3.9
- SQLite 3.39
- Streamlit 1.40

æ•°æ®è§„æ¨¡ï¼š
- æ”¿ç­–æ–‡æ¡£: 50ä¸ª
- å›¾è°±èŠ‚ç‚¹: 500ä¸ª
- å›¾è°±è¾¹: 800æ¡
```

### å…³é”®æŒ‡æ ‡

**APIè°ƒç”¨**:
```python
# RAGFlowæœç´¢
å¹³å‡è€—æ—¶: 300ms
P95: 500ms
P99: 800ms

# Qwenå®ä½“æŠ½å–
å¹³å‡è€—æ—¶: 2.5s
P95: 4s
P99: 6s
```

**æ•°æ®åº“æŸ¥è¯¢**:
```sql
-- ç®€å•æŸ¥è¯¢ (< 10ms)
SELECT * FROM policies WHERE id = 1;

-- JOINæŸ¥è¯¢ (< 50ms)
SELECT p.*, t.name 
FROM policies p 
JOIN policy_tags pt ON p.id = pt.policy_id
JOIN tags t ON pt.tag_id = t.id;

-- å¤æ‚å›¾è°±æŸ¥è¯¢ (< 200ms)
SELECT source, target, relation FROM graph_edges WHERE ragflow_doc_id = 'doc_123';
```

---

## ç¼“å­˜ç­–ç•¥

### 1. Session Stateç¼“å­˜ï¼ˆStreamlitï¼‰

**å›¾è°±æ•°æ®ç¼“å­˜**:

```python
import streamlit as st

# åˆå§‹åŒ–session state
if 'graph_data' not in st.session_state:
    st.session_state.graph_data = None

# ä½¿ç”¨ç¼“å­˜
def get_cached_graph():
    """è·å–ç¼“å­˜çš„å›¾è°±æ•°æ®"""
    if st.session_state.graph_data is None:
        # é¦–æ¬¡åŠ è½½ï¼Œä»æ•°æ®åº“è¯»å–
        graph_dao = GraphDAO()
        nodes = graph_dao.get_all_nodes()
        edges = graph_dao.get_all_edges()
        st.session_state.graph_data = {'nodes': nodes, 'edges': edges}
    
    return st.session_state.graph_data
```

**æ•ˆæœ**:
- é¦–æ¬¡åŠ è½½: 2s
- åç»­è®¿é—®: < 50msï¼ˆ40å€æå‡ï¼‰

### 2. RAGFlowå®¢æˆ·ç«¯ç¼“å­˜

**Datasetå¯¹è±¡ç¼“å­˜**:

```python
class RAGFlowClient:
    def __init__(self):
        self._dataset_cache = {}  # çŸ¥è¯†åº“ç¼“å­˜
        self._chat_cache = {}     # èŠå¤©åŠ©æ‰‹ç¼“å­˜
    
    def _get_or_create_dataset(self, kb_name: str):
        """è·å–æˆ–ç¼“å­˜æ•°æ®é›†å¯¹è±¡"""
        if kb_name in self._dataset_cache:
            return self._dataset_cache[kb_name]  # å‘½ä¸­ç¼“å­˜
        
        # æœªå‘½ä¸­ï¼ŒæŸ¥è¯¢å¹¶ç¼“å­˜
        datasets = self.rag.list_datasets(name=kb_name)
        if datasets:
            self._dataset_cache[kb_name] = datasets[0]
            return datasets[0]
        
        return None
```

**æ•ˆæœ**:
- æ— ç¼“å­˜: 500ms/æ¬¡
- æœ‰ç¼“å­˜: 5ms/æ¬¡ï¼ˆ100å€æå‡ï¼‰

### 3. LRUç¼“å­˜ï¼ˆå‡½æ•°çº§ï¼‰

```python
from functools import lru_cache

@lru_cache(maxsize=128)
def get_policy_by_id(policy_id: int):
    """å¸¦ç¼“å­˜çš„æ”¿ç­–æŸ¥è¯¢"""
    dao = PolicyDAO()
    return dao.get_policy_by_id(policy_id)

# ä½¿ç”¨
policy = get_policy_by_id(1)  # é¦–æ¬¡æŸ¥è¯¢æ•°æ®åº“
policy = get_policy_by_id(1)  # å‘½ä¸­ç¼“å­˜ï¼Œä¸æŸ¥æ•°æ®åº“
```

**é€‚ç”¨åœºæ™¯**:
- é«˜é¢‘è®¿é—®çš„ä¸å¯å˜æ•°æ®
- è®¡ç®—å¯†é›†å‹å‡½æ•°ç»“æœ

### 4. æ–‡ä»¶ç¼“å­˜

```python
import pickle
from pathlib import Path

class CacheManager:
    """æ–‡ä»¶ç¼“å­˜ç®¡ç†å™¨"""
    
    def __init__(self, cache_dir: str = "data/cache"):
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(exist_ok=True)
    
    def get(self, key: str):
        """è·å–ç¼“å­˜"""
        cache_file = self.cache_dir / f"{key}.pkl"
        if cache_file.exists():
            with open(cache_file, 'rb') as f:
                return pickle.load(f)
        return None
    
    def set(self, key: str, value):
        """è®¾ç½®ç¼“å­˜"""
        cache_file = self.cache_dir / f"{key}.pkl"
        with open(cache_file, 'wb') as f:
            pickle.dump(value, f)

# ä½¿ç”¨
cache = CacheManager()
graph_data = cache.get('graph_full')
if graph_data is None:
    graph_data = build_graph()  # è€—æ—¶æ“ä½œ
    cache.set('graph_full', graph_data)
```

---

## æ‰¹é‡å¤„ç†

### 1. æ‰¹é‡æ•°æ®åº“å†™å…¥

**âŒ ä½æ•ˆï¼ˆé€æ¡æ’å…¥ï¼‰**:
```python
for entity in entities:
    graph_dao.add_node(entity['name'], entity['type'])
# è€—æ—¶: 500ä¸ªå®ä½“ Ã— 10ms = 5ç§’
```

**âœ… é«˜æ•ˆï¼ˆæ‰¹é‡æ’å…¥ï¼‰**:
```python
def batch_add_nodes(nodes: List[Dict]):
    """æ‰¹é‡æ·»åŠ èŠ‚ç‚¹"""
    conn = get_db_connection()
    cursor = conn.cursor()
    
    # ä½¿ç”¨executemanyæ‰¹é‡æ’å…¥
    cursor.executemany(
        "INSERT OR IGNORE INTO graph_nodes (label, type) VALUES (?, ?)",
        [(node['name'], node['type']) for node in nodes]
    )
    
    conn.commit()
    conn.close()

batch_add_nodes(entities)
# è€—æ—¶: < 100msï¼ˆ50å€æå‡ï¼‰
```

### 2. æ‰¹é‡APIè°ƒç”¨

**åˆ†å—å¤„ç†æ–‡æ¡£**:
```python
def batch_extract_entities(chunks: List[str], doc_title: str, batch_size: int = 5):
    """æ‰¹é‡æå–å®ä½“ï¼ˆåˆ†æ‰¹å¤„ç†ï¼‰"""
    all_entities = []
    all_relations = []
    
    for i in range(0, len(chunks), batch_size):
        batch = chunks[i:i+batch_size]
        
        # åˆå¹¶æ–‡æœ¬ï¼ˆå‡å°‘APIè°ƒç”¨æ¬¡æ•°ï¼‰
        combined_text = "\n\n".join(batch)
        
        # è°ƒç”¨Qwen API
        result = qwen_client.extract_entities_and_relations(combined_text, doc_title)
        
        all_entities.extend(result['entities'])
        all_relations.extend(result['relations'])
    
    return {'entities': all_entities, 'relations': all_relations}
```

**æ•ˆæœ**:
- 10ä¸ªchunkç‹¬ç«‹è°ƒç”¨: 10 Ã— 2.5s = 25s
- æ‰¹é‡åˆå¹¶è°ƒç”¨: 2æ¬¡ Ã— 3s = 6sï¼ˆ4å€æå‡ï¼‰

---

## å¼‚æ­¥ä¼˜åŒ–

### 1. å¼‚æ­¥æ–‡æ¡£ä¸Šä¼ 

```python
import asyncio

async def upload_document_async(file_path: str):
    """å¼‚æ­¥ä¸Šä¼ æ–‡æ¡£"""
    loop = asyncio.get_event_loop()
    return await loop.run_in_executor(
        None,
        ragflow_client.upload_document,
        file_path
    )

async def batch_upload_async(file_paths: List[str]):
    """å¹¶å‘ä¸Šä¼ å¤šä¸ªæ–‡æ¡£"""
    tasks = [upload_document_async(fp) for fp in file_paths]
    results = await asyncio.gather(*tasks)
    return results

# ä½¿ç”¨
asyncio.run(batch_upload_async(['file1.pdf', 'file2.pdf', 'file3.pdf']))
```

**æ•ˆæœ**:
- ä¸²è¡Œä¸Šä¼ 3ä¸ªæ–‡ä»¶: 3 Ã— 3s = 9s
- å¹¶å‘ä¸Šä¼ : ~4sï¼ˆ2å€æå‡ï¼‰

### 2. å¼‚æ­¥æ•°æ®åº“æŸ¥è¯¢

```python
import aiosqlite

async def get_policies_async():
    """å¼‚æ­¥æŸ¥è¯¢æ”¿ç­–"""
    async with aiosqlite.connect('data/database/policies.db') as db:
        async with db.execute('SELECT * FROM policies') as cursor:
            rows = await cursor.fetchall()
            return rows
```

---

## æ•°æ®åº“ä¼˜åŒ–

### 1. è¿æ¥æ± 

```python
from contextlib import contextmanager
import threading

class ConnectionPool:
    """SQLiteè¿æ¥æ± """
    
    def __init__(self, db_path: str, max_connections: int = 5):
        self.db_path = db_path
        self.max_connections = max_connections
        self.pool = []
        self.lock = threading.Lock()
    
    @contextmanager
    def get_connection(self):
        """è·å–è¿æ¥"""
        with self.lock:
            if self.pool:
                conn = self.pool.pop()
            else:
                conn = sqlite3.connect(self.db_path)
        
        try:
            yield conn
        finally:
            with self.lock:
                if len(self.pool) < self.max_connections:
                    self.pool.append(conn)
                else:
                    conn.close()
```

### 2. æŸ¥è¯¢ä¼˜åŒ–

**ä½¿ç”¨ç´¢å¼•**:
```sql
-- åˆ›å»ºå¤åˆç´¢å¼•
CREATE INDEX idx_graph_edges_doc_relation 
ON graph_edges(ragflow_doc_id, relation);

-- æŸ¥è¯¢è‡ªåŠ¨ä½¿ç”¨ç´¢å¼•
SELECT * FROM graph_edges 
WHERE ragflow_doc_id = 'doc_123' AND relation = 'ISSUED_BY';
```

**é¿å…å…¨è¡¨æ‰«æ**:
```sql
-- âŒ å…¨è¡¨æ‰«æ
SELECT * FROM policies WHERE LOWER(title) LIKE '%å€ºåˆ¸%';

-- âœ… ä½¿ç”¨ç´¢å¼•ï¼ˆå¦‚æœæœ‰ï¼‰
SELECT * FROM policies WHERE title LIKE 'å€ºåˆ¸%';
```

### 3. é¢„ç¼–è¯‘è¯­å¥

```python
conn = sqlite3.connect('policies.db')

# é¢„ç¼–è¯‘æŸ¥è¯¢
stmt = conn.execute("SELECT * FROM policies WHERE id = ?")

# é‡å¤ä½¿ç”¨
for policy_id in [1, 2, 3, 4, 5]:
    cursor = stmt.execute([policy_id])
    result = cursor.fetchone()
```

---

## å‰ç«¯ä¼˜åŒ–

### 1. å»¶è¿ŸåŠ è½½

```python
import streamlit as st

# ä½¿ç”¨st.spinneræ˜¾ç¤ºåŠ è½½çŠ¶æ€
with st.spinner('åŠ è½½å›¾è°±æ•°æ®...'):
    graph_data = get_cached_graph()

# å»¶è¿ŸåŠ è½½å¤§å‹ç»„ä»¶
if st.button('æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯'):
    st.write(detailed_data)  # åªåœ¨ç‚¹å‡»æ—¶åŠ è½½
```

### 2. åˆ†é¡µæ˜¾ç¤º

```python
def paginate_data(data: List, page_size: int = 20):
    """åˆ†é¡µæ˜¾ç¤ºæ•°æ®"""
    total_pages = (len(data) - 1) // page_size + 1
    
    page = st.number_input('é¡µç ', min_value=1, max_value=total_pages, value=1)
    
    start = (page - 1) * page_size
    end = start + page_size
    
    return data[start:end]

# ä½¿ç”¨
policies = get_all_policies()  # å‡è®¾æœ‰1000æ¡
display_policies = paginate_data(policies, page_size=20)
st.dataframe(display_policies)
```

### 3. è™šæ‹ŸåŒ–åˆ—è¡¨

```python
# å¯¹äºè¶…å¤§æ•°æ®é›†ï¼Œåªæ¸²æŸ“å¯è§éƒ¨åˆ†
def render_virtual_list(items: List, viewport_size: int = 10):
    """è™šæ‹ŸåŒ–åˆ—è¡¨æ¸²æŸ“"""
    scroll_position = st.slider('æ»šåŠ¨', 0, len(items) - viewport_size, 0)
    
    visible_items = items[scroll_position:scroll_position + viewport_size]
    
    for item in visible_items:
        st.write(item)
```

---

## ç›‘æ§å’Œåˆ†æ

### 1. æ€§èƒ½ç›‘æ§

```python
import time
import logging

logger = logging.getLogger(__name__)

def performance_monitor(func):
    """æ€§èƒ½ç›‘æ§è£…é¥°å™¨"""
    def wrapper(*args, **kwargs):
        start_time = time.time()
        
        try:
            result = func(*args, **kwargs)
            elapsed = time.time() - start_time
            
            # è®°å½•æ€§èƒ½æŒ‡æ ‡
            logger.info(f"{func.__name__} è€—æ—¶: {elapsed:.2f}ç§’")
            
            # æ€§èƒ½å‘Šè­¦ï¼ˆè¶…è¿‡é˜ˆå€¼ï¼‰
            if elapsed > 5:
                logger.warning(f"{func.__name__} å“åº”æ…¢: {elapsed:.2f}ç§’")
            
            return result
        except Exception as e:
            elapsed = time.time() - start_time
            logger.error(f"{func.__name__} å¤±è´¥: {e}, è€—æ—¶: {elapsed:.2f}ç§’")
            raise
    
    return wrapper

# ä½¿ç”¨
@performance_monitor
def build_graph_for_document(doc_id: str):
    # æ„å»ºå›¾è°±é€»è¾‘
    pass
```

### 2. å†…å­˜ç›‘æ§

```python
import tracemalloc

def memory_monitor(func):
    """å†…å­˜ç›‘æ§è£…é¥°å™¨"""
    def wrapper(*args, **kwargs):
        tracemalloc.start()
        
        result = func(*args, **kwargs)
        
        current, peak = tracemalloc.get_traced_memory()
        tracemalloc.stop()
        
        logger.info(f"{func.__name__} å†…å­˜: å½“å‰={current/1024/1024:.2f}MB, å³°å€¼={peak/1024/1024:.2f}MB")
        
        return result
    
    return wrapper
```

### 3. Streamlitæ€§èƒ½åˆ†æ

```python
import streamlit as st

# å¯ç”¨æ€§èƒ½åˆ†æ
st.set_page_config(
    page_title="æ”¿ç­–åº“ç³»ç»Ÿ",
    layout="wide",
    initial_sidebar_state="expanded"
)

# ä½¿ç”¨st.cache_dataç¼“å­˜æ•°æ®
@st.cache_data
def load_graph_data():
    """ç¼“å­˜å›¾è°±æ•°æ®"""
    return get_all_graph_data()

# ä½¿ç”¨st.cache_resourceç¼“å­˜èµ„æº
@st.cache_resource
def get_ragflow_client():
    """ç¼“å­˜RAGFlowå®¢æˆ·ç«¯"""
    return RAGFlowClient()
```

---

## æœ€ä½³å®è·µ

### 1. é€‰æ‹©åˆé€‚çš„ç¼“å­˜ç­–ç•¥

| æ•°æ®ç±»å‹ | æ¨èç¼“å­˜ | ç¼“å­˜æ—¶é•¿ |
|---------|---------|---------|
| é™æ€é…ç½® | @lru_cache | æ°¸ä¹… |
| å›¾è°±æ•°æ® | Session State | ä¼šè¯æœŸé—´ |
| APIå®¢æˆ·ç«¯ | @st.cache_resource | æ°¸ä¹… |
| æŸ¥è¯¢ç»“æœ | @st.cache_data(ttl=300) | 5åˆ†é’Ÿ |

### 2. é¿å…è¿‡æ—©ä¼˜åŒ–

```
1. å…ˆå®ç°åŠŸèƒ½
2. æµ‹è¯•æ€§èƒ½
3. è¯†åˆ«ç“¶é¢ˆï¼ˆä½¿ç”¨profilerï¼‰
4. é’ˆå¯¹æ€§ä¼˜åŒ–
5. éªŒè¯ä¼˜åŒ–æ•ˆæœ
```

### 3. æ€§èƒ½vså¯è¯»æ€§æƒè¡¡

```python
# âœ… æ¸…æ™°ä½†ç¨æ…¢
for entity in entities:
    if entity['type'] == 'POLICY':
        process_policy(entity)

# âš ï¸ æ›´å¿«ä½†éš¾è¯»
list(map(process_policy, filter(lambda e: e['type'] == 'POLICY', entities)))

# ç»“è®ºï¼šä¼˜å…ˆé€‰æ‹©æ¸…æ™°ä»£ç ï¼Œé™¤éæ€§èƒ½ç“¶é¢ˆæ˜ç¡®
```

### 4. å®šæœŸæ€§èƒ½å›å½’æµ‹è¯•

```python
import pytest
import time

def test_search_performance():
    """æœç´¢æ€§èƒ½æµ‹è¯•"""
    start = time.time()
    
    results = search_service.search("ä¸“é¡¹å€ºåˆ¸", top_k=10)
    
    elapsed = time.time() - start
    
    # æ–­è¨€å“åº”æ—¶é—´ < 500ms
    assert elapsed < 0.5, f"æœç´¢è€—æ—¶è¿‡é•¿: {elapsed:.2f}ç§’"
    assert len(results) <= 10
```

---

## ç›¸å…³æ–‡æ¡£

- [ç³»ç»Ÿæ¶æ„](../02-ARCHITECTURE.md) - äº†è§£ç³»ç»Ÿæ•´ä½“æ€§èƒ½è®¾è®¡
- [æ•°æ®åº“è®¾è®¡](database-schema.md) - æ•°æ®åº“ç´¢å¼•å’Œä¼˜åŒ–
- [RAGFlowé›†æˆ](ragflow-integration.md) - RAGFlowæ€§èƒ½ä¼˜åŒ–
- [Qwené›†æˆ](qwen-integration.md) - Qwenè°ƒç”¨ä¼˜åŒ–

---

**æœ€åæ›´æ–°**: 2026-02-01  
**ç»´æŠ¤è€…**: AI Assistant
