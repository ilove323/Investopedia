# ğŸ“š API å‚è€ƒæ–‡æ¡£

> æ‰€æœ‰æœåŠ¡ã€DAOã€ä¸šåŠ¡é€»è¾‘ç±»çš„è¯¦ç»†APIæ–‡æ¡£  
> é˜…è¯»æ—¶é—´: 45åˆ†é’Ÿ

---

## ğŸ“‹ ç›®å½•

- [æœåŠ¡å±‚API](#æœåŠ¡å±‚api)
- [æ•°æ®è®¿é—®å±‚API](#æ•°æ®è®¿é—®å±‚api)
- [ä¸šåŠ¡é€»è¾‘å±‚API](#ä¸šåŠ¡é€»è¾‘å±‚api)
- [æ¨¡å‹ç±»API](#æ¨¡å‹ç±»api)
- [å·¥å…·å‡½æ•°API](#å·¥å…·å‡½æ•°api)

---

## ğŸ”Œ æœåŠ¡å±‚API

### RAGFlowClient

**æ¨¡å—**: `src/services/ragflow_client.py`

#### åˆå§‹åŒ–
```python
from src.clients.ragflow_client import get_ragflow_client

client = get_ragflow_client()
```

#### check_health()
```python
def check_health() -> Dict
```

æ£€æŸ¥RAGFlowæœåŠ¡å¥åº·çŠ¶æ€ã€‚

**è¿”å›å€¼**:
```python
{
    "status": "healthy",  # or "unhealthy"
    "version": "0.13.0",
    "timestamp": "2026-02-01T10:30:00Z"
}
```

**ç¤ºä¾‹**:
```python
health = client.check_health()
if health['status'] == 'healthy':
    print("RAGFlowæœåŠ¡æ­£å¸¸")
```

---

#### get_documents()
```python
def get_documents(kb_name: str) -> List[Dict]
```

è·å–æŒ‡å®šçŸ¥è¯†åº“çš„æ‰€æœ‰æ–‡æ¡£åˆ—è¡¨ã€‚

**å‚æ•°**:
- `kb_name` (str): çŸ¥è¯†åº“åç§°

**è¿”å›å€¼**:
```python
[
    {
        "id": "doc_123",
        "name": "æ”¿ç­–æ–‡æ¡£.pdf",
        "size": 1024000,  # å­—èŠ‚
        "chunk_count": 45,
        "token_count": 12345,
        "created_at": "2024-01-15T10:30:00Z",
        "status": "completed"
    },
    ...
]
```

**å¼‚å¸¸**:
- `ConnectionError`: æ— æ³•è¿æ¥åˆ°RAGFlow
- `ValueError`: çŸ¥è¯†åº“ä¸å­˜åœ¨

**ç¤ºä¾‹**:
```python
docs = client.get_documents("policy_demo_kb")
for doc in docs:
    print(f"{doc['name']}: {doc['chunk_count']} chunks")
```

---

#### get_document_content()
```python
def get_document_content(doc_id: str, kb_name: str) -> str
```

è·å–æ–‡æ¡£çš„å®Œæ•´å†…å®¹ï¼ˆæ‰€æœ‰chunksæ‹¼æ¥ï¼‰ã€‚

**å‚æ•°**:
- `doc_id` (str): æ–‡æ¡£ID
- `kb_name` (str): çŸ¥è¯†åº“åç§°

**è¿”å›å€¼**:
- `str`: æ–‡æ¡£å®Œæ•´æ–‡æœ¬å†…å®¹

**å¼‚å¸¸**:
- `DocumentNotFoundError`: æ–‡æ¡£ä¸å­˜åœ¨
- `ConnectionError`: ç½‘ç»œé”™è¯¯

**ç¤ºä¾‹**:
```python
content = client.get_document_content("doc_123", "policy_demo_kb")
print(f"æ–‡æ¡£é•¿åº¦: {len(content)} å­—ç¬¦")
```

---

#### retrieve()
```python
def retrieve(question: str, kb_name: str, 
            top_k: int = 5, 
            similarity_threshold: float = 0.3) -> List[Dict]
```

å‘é‡æ£€ç´¢ç›¸å…³æ–‡æ¡£ã€‚

**å‚æ•°**:
- `question` (str): æŸ¥è¯¢é—®é¢˜
- `kb_name` (str): çŸ¥è¯†åº“åç§°
- `top_k` (int, å¯é€‰): è¿”å›Top-Kä¸ªæ–‡æ¡£ï¼Œé»˜è®¤5
- `similarity_threshold` (float, å¯é€‰): ç›¸ä¼¼åº¦é˜ˆå€¼ï¼Œé»˜è®¤0.3

**è¿”å›å€¼**:
```python
[
    {
        "doc_id": "doc_123",
        "doc_name": "æ”¿ç­–æ–‡æ¡£.pdf",
        "chunk_id": "chunk_45",
        "content": "ç›¸å…³å†…å®¹ç‰‡æ®µ...",
        "similarity": 0.92,
        "metadata": {...}
    },
    ...
]
```

**ç¤ºä¾‹**:
```python
results = client.retrieve(
    "é«˜æ–°æŠ€æœ¯ä¼ä¸šç¨æ”¶ä¼˜æƒ ",
    "policy_demo_kb",
    top_k=10,
    similarity_threshold=0.5
)

for r in results:
    print(f"{r['doc_name']} (ç›¸ä¼¼åº¦: {r['similarity']:.2f})")
    print(r['content'][:100])
```

---

#### list_datasets()
```python
def list_datasets() -> List[str]
```

åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„çŸ¥è¯†åº“ã€‚

**è¿”å›å€¼**:
- `List[str]`: çŸ¥è¯†åº“åç§°åˆ—è¡¨

**ç¤ºä¾‹**:
```python
kbs = client.list_datasets()
print(f"å¯ç”¨çŸ¥è¯†åº“: {', '.join(kbs)}")
```

---

### QwenClient

**æ¨¡å—**: `src/services/qwen_client.py`

#### åˆå§‹åŒ–
```python
from src.clients.qwen_client import get_qwen_client

client = get_qwen_client()
```

#### extract_entities_and_relations()
```python
def extract_entities_and_relations(text: str, 
                                   doc_title: str = "") -> Dict
```

ä»æ–‡æœ¬ä¸­æŠ½å–å®ä½“å’Œå…³ç³»ã€‚

**å‚æ•°**:
- `text` (str): å¾…æŠ½å–çš„æ–‡æœ¬å†…å®¹
- `doc_title` (str, å¯é€‰): æ–‡æ¡£æ ‡é¢˜ï¼ˆä½œä¸ºä¸Šä¸‹æ–‡ï¼‰

**è¿”å›å€¼**:
```python
{
    "entities": [
        {
            "text": "å¹¿ä¸œçœç§‘æŠ€å…",
            "type": "AUTHORITY",
            "description": "æ”¿ç­–å‘å¸ƒæœºæ„"
        },
        {
            "text": "é«˜æ–°æŠ€æœ¯ä¼ä¸š",
            "type": "CONCEPT",
            "description": "æ”¿ç­–é€‚ç”¨å¯¹è±¡"
        }
    ],
    "relations": [
        {
            "source": "å¹¿ä¸œçœç§‘æŠ€å…",
            "target": "ç§‘æŠ€åˆ›æ–°æ”¿ç­–",
            "type": "ISSUED_BY"
        },
        {
            "source": "ç§‘æŠ€åˆ›æ–°æ”¿ç­–",
            "target": "é«˜æ–°æŠ€æœ¯ä¼ä¸š",
            "type": "APPLIES_TO"
        }
    ]
}
```

**å®ä½“ç±»å‹**:
- `POLICY`: æ”¿ç­–æ–‡æ¡£
- `AUTHORITY`: å‘å¸ƒæœºæ„
- `REGION`: åœ°åŒº
- `CONCEPT`: æ¦‚å¿µ/é¢†åŸŸ
- `PROJECT`: é¡¹ç›®/è®¡åˆ’

**å…³ç³»ç±»å‹**:
- `ISSUED_BY`: å‘å¸ƒå…³ç³»
- `APPLIES_TO`: é€‚ç”¨å…³ç³»
- `REFERENCES`: å¼•ç”¨å…³ç³»
- `AFFECTS`: å½±å“å…³ç³»
- `BELONGS_TO`: ä»å±å…³ç³»

**å¼‚å¸¸**:
- `APIError`: APIè°ƒç”¨å¤±è´¥
- `InvalidResponseError`: è¿”å›æ ¼å¼é”™è¯¯

**ç¤ºä¾‹**:
```python
text = """
å¹¿ä¸œçœç§‘æŠ€å…å‘å¸ƒã€Šç§‘æŠ€åˆ›æ–°æ”¿ç­–ã€‹ï¼Œé€‚ç”¨äºé«˜æ–°æŠ€æœ¯ä¼ä¸šã€‚
ä¼ä¸šç ”å‘è´¹ç”¨å¯äº«å—åŠ è®¡æ‰£é™¤ä¼˜æƒ ã€‚
"""

result = client.extract_entities_and_relations(text, "ç§‘æŠ€åˆ›æ–°æ”¿ç­–")

print(f"æå–å®ä½“æ•°: {len(result['entities'])}")
print(f"æå–å…³ç³»æ•°: {len(result['relations'])}")

for entity in result['entities']:
    print(f"- {entity['text']} ({entity['type']})")
```

**æ€§èƒ½**:
- å•æ¬¡è°ƒç”¨è€—æ—¶: 3-5ç§’
- Tokenæ¶ˆè€—: ~1500-3000 (æ ¹æ®æ–‡æ¡£é•¿åº¦)
- æˆæœ¬: ~ï¿¥0.01-0.02/æ–‡æ¡£ (qwen-plus)

---

### ChatService

**æ¨¡å—**: `src/services/chat_service.py`

#### åˆå§‹åŒ–
```python
from src.services.chat_service import get_chat_service

service = get_chat_service()
```

#### chat()
```python
def chat(question: str, 
        session_id: str = None,
        stream: bool = False) -> Dict
```

å‘é€é—®é¢˜åˆ°Chat Assistantå¹¶è·å–ç­”æ¡ˆã€‚

**å‚æ•°**:
- `question` (str): ç”¨æˆ·é—®é¢˜
- `session_id` (str, å¯é€‰): ä¼šè¯IDï¼ŒNoneåˆ™åˆ›å»ºæ–°ä¼šè¯
- `stream` (bool, å¯é€‰): æ˜¯å¦æµå¼è¾“å‡ºï¼Œé»˜è®¤False

**è¿”å›å€¼ï¼ˆéæµå¼ï¼‰**:
```python
{
    "answer": "å®Œæ•´ç­”æ¡ˆæ–‡æœ¬...",
    "references": [
        {
            "doc_id": "doc_123",
            "doc_name": "æ”¿ç­–æ–‡æ¡£.pdf",
            "chunk_id": "chunk_45",
            "similarity": 0.92
        }
    ],
    "session_id": "session_abc123"
}
```

**è¿”å›å€¼ï¼ˆæµå¼ï¼‰**:
```python
# ç”Ÿæˆå™¨ï¼Œé€å—è¿”å›
for chunk in service.chat("é—®é¢˜", stream=True):
    print(chunk['delta'])  # å¢é‡æ–‡æœ¬
    # æœ€åä¸€ä¸ªchunkåŒ…å«å®Œæ•´answerå’Œreferences
```

**ç¤ºä¾‹ï¼ˆéæµå¼ï¼‰**:
```python
response = service.chat("é«˜æ–°æŠ€æœ¯ä¼ä¸šæœ‰å“ªäº›ç¨æ”¶ä¼˜æƒ ï¼Ÿ")
print(response['answer'])
for ref in response['references']:
    print(f"å‚è€ƒ: {ref['doc_name']}")
```

**ç¤ºä¾‹ï¼ˆæµå¼ï¼‰**:
```python
import streamlit as st

message_placeholder = st.empty()
full_response = ""

for chunk in service.chat("é—®é¢˜", stream=True):
    if 'delta' in chunk:
        full_response += chunk['delta']
        message_placeholder.write(full_response)
    elif 'answer' in chunk:
        # æœ€åä¸€ä¸ªchunk
        full_response = chunk['answer']
        references = chunk['references']
```

---

#### create_session()
```python
def create_session(kb_name: str = None) -> str
```

åˆ›å»ºæ–°çš„å¯¹è¯ä¼šè¯ã€‚

**å‚æ•°**:
- `kb_name` (str, å¯é€‰): æŒ‡å®šçŸ¥è¯†åº“ï¼ŒNoneåˆ™ä½¿ç”¨é»˜è®¤

**è¿”å›å€¼**:
- `str`: ä¼šè¯ID

**ç¤ºä¾‹**:
```python
session_id = service.create_session("policy_demo_kb")
response = service.chat("é—®é¢˜1", session_id=session_id)
response = service.chat("é—®é¢˜2", session_id=session_id)  # è®°ä½ä¸Šä¸‹æ–‡
```

---

#### list_sessions()
```python
def list_sessions() -> List[Dict]
```

åˆ—å‡ºæ‰€æœ‰ä¼šè¯ã€‚

**è¿”å›å€¼**:
```python
[
    {
        "session_id": "session_abc123",
        "created_at": "2026-02-01T10:00:00Z",
        "message_count": 10,
        "last_message_at": "2026-02-01T10:30:00Z"
    },
    ...
]
```

---

### DataSyncService

**æ¨¡å—**: `src/services/data_sync.py`

#### åˆå§‹åŒ–
```python
from src.services.data_sync import DataSyncService

service = DataSyncService()
```

#### sync_documents_to_database()
```python
def sync_documents_to_database(kb_name: str) -> Dict
```

ä»RAGFlowåŒæ­¥æ–‡æ¡£å…ƒæ•°æ®åˆ°æœ¬åœ°SQLiteã€‚

**å‚æ•°**:
- `kb_name` (str): çŸ¥è¯†åº“åç§°

**è¿”å›å€¼**:
```python
{
    "synced_count": 10,    # æ–°å¢/æ›´æ–°æ–‡æ¡£æ•°
    "skipped_count": 5,    # è·³è¿‡ï¼ˆå·²å­˜åœ¨ä¸”æœªä¿®æ”¹ï¼‰
    "total_count": 15,     # æ€»æ–‡æ¡£æ•°
    "elapsed_time": 3.2    # è€—æ—¶ï¼ˆç§’ï¼‰
}
```

**ç¤ºä¾‹**:
```python
result = service.sync_documents_to_database("policy_demo_kb")
print(f"åŒæ­¥äº† {result['synced_count']} ä¸ªæ–‡æ¡£")
```

---

#### build_knowledge_graph()
```python
def build_knowledge_graph(kb_name: str, 
                         is_incremental: bool = False) -> Dict
```

æ„å»ºçŸ¥è¯†å›¾è°±ï¼ˆ**æ ¸å¿ƒæ–¹æ³•**ï¼‰ã€‚

**å‚æ•°**:
- `kb_name` (str): çŸ¥è¯†åº“åç§°
- `is_incremental` (bool, å¯é€‰): æ˜¯å¦å¢é‡æ„å»ºï¼Œé»˜è®¤Falseï¼ˆå…¨é‡ï¼‰

**è¿”å›å€¼**:
```python
{
    "node_count": 40,
    "edge_count": 73,
    "document_count": 12,
    "elapsed_time": 145.6,  # ç§’
    "errors": []  # é”™è¯¯åˆ—è¡¨ï¼ˆå¦‚æœæœ‰ï¼‰
}
```

**æµç¨‹**:
1. è·å–RAGFlowæ–‡æ¡£åˆ—è¡¨
2. éå†æ¯ä¸ªæ–‡æ¡£ï¼š
   - è·å–æ–‡æ¡£å†…å®¹
   - è°ƒç”¨QwenæŠ½å–å®ä½“å’Œå…³ç³»
   - æ„å»ºGraphNodeå’ŒGraphEdge
3. å»é‡ï¼ˆæ–‡æ¡£åã€èŠ‚ç‚¹IDï¼‰
4. ä¿å­˜åˆ°SQLite

**æ€§èƒ½**:
- å…¨é‡æ„å»º: ~3-5åˆ†é’Ÿ (40æ–‡æ¡£)
- å¢é‡æ„å»º: <1åˆ†é’Ÿ (5ä¸ªæ–°æ–‡æ¡£)

**ç¤ºä¾‹**:
```python
# å…¨é‡æ„å»º
result = service.build_knowledge_graph("policy_demo_kb", is_incremental=False)
print(f"æ„å»ºå®Œæˆ: {result['node_count']}èŠ‚ç‚¹, {result['edge_count']}è¾¹")

# å¢é‡æ„å»ºï¼ˆåªå¤„ç†æ–°æ–‡æ¡£ï¼‰
result = service.build_knowledge_graph("policy_demo_kb", is_incremental=True)
```

---

#### get_sync_status()
```python
def get_sync_status() -> Dict
```

è·å–å½“å‰åŒæ­¥çŠ¶æ€ã€‚

**è¿”å›å€¼**:
```python
{
    "is_syncing": False,
    "last_sync_time": "2026-02-01T10:00:00Z",
    "current_progress": 0.0,  # 0.0-1.0
    "current_document": ""
}
```

---

### WhisperClient

**æ¨¡å—**: `src/services/whisper_client.py`

#### transcribe()
```python
def transcribe(audio_file: Union[str, BinaryIO]) -> str
```

è½¬å½•éŸ³é¢‘æ–‡ä»¶ä¸ºæ–‡æœ¬ã€‚

**å‚æ•°**:
- `audio_file`: æ–‡ä»¶è·¯å¾„ï¼ˆstrï¼‰æˆ–æ–‡ä»¶å¯¹è±¡

**è¿”å›å€¼**:
- `str`: è½¬å½•æ–‡æœ¬

**æ”¯æŒæ ¼å¼**: WAV, MP3, M4A, FLAC, OGG

**æ–‡ä»¶å¤§å°é™åˆ¶**: 25MB

**ç¤ºä¾‹**:
```python
from src.services.whisper_client import get_whisper_client

client = get_whisper_client()

# æ–¹å¼1: æ–‡ä»¶è·¯å¾„
text = client.transcribe("audio/recording.mp3")

# æ–¹å¼2: æ–‡ä»¶å¯¹è±¡
with open("audio/recording.mp3", "rb") as f:
    text = client.transcribe(f)

print(f"è½¬å½•ç»“æœ: {text}")
```

---

## ğŸ—„ï¸ æ•°æ®è®¿é—®å±‚API

### PolicyDAO

**æ¨¡å—**: `src/database/policy_dao.py`

#### åˆå§‹åŒ–
```python
from src.database.policy_dao import get_policy_dao

dao = get_policy_dao()
```

#### create_policy()
```python
def create_policy(metadata: Dict) -> int
```

åˆ›å»ºæ–°æ”¿ç­–è®°å½•ã€‚

**å‚æ•°**:
```python
metadata = {
    "ragflow_id": "doc_123",
    "title": "ç§‘æŠ€åˆ›æ–°æ”¿ç­–",
    "policy_type": "ç§‘æŠ€æ”¿ç­–",
    "region": "å¹¿ä¸œ",
    "issuing_authority": "å¹¿ä¸œçœç§‘æŠ€å…",
    "document_number": "ç²¤ç§‘ã€”2024ã€•1å·",
    "effective_date": "2024-01-01",
    "expiry_date": "2025-12-31",
    "status": "æœ‰æ•ˆ",
    "content": "æ”¿ç­–å…¨æ–‡...",
    "summary": "æ”¿ç­–æ‘˜è¦..."
    # ...æ›´å¤šå­—æ®µè§schema.sql
}
```

**è¿”å›å€¼**:
- `int`: æ–°åˆ›å»ºæ”¿ç­–çš„ID

**ç¤ºä¾‹**:
```python
policy_id = dao.create_policy({
    "ragflow_id": "doc_new",
    "title": "æ–°æ”¿ç­–",
    "policy_type": "è´¢ç¨æ”¿ç­–",
    # ...
})
print(f"åˆ›å»ºæˆåŠŸï¼ŒID: {policy_id}")
```

---

#### get_policy_by_ragflow_id()
```python
def get_policy_by_ragflow_id(doc_id: str) -> Optional[Policy]
```

æ ¹æ®RAGFlowæ–‡æ¡£IDæŸ¥è¯¢æ”¿ç­–ã€‚

**å‚æ•°**:
- `doc_id` (str): RAGFlowæ–‡æ¡£ID

**è¿”å›å€¼**:
- `Policy`: Policyå¯¹è±¡ï¼Œä¸å­˜åœ¨è¿”å›None

**ç¤ºä¾‹**:
```python
policy = dao.get_policy_by_ragflow_id("doc_123")
if policy:
    print(f"æ”¿ç­–æ ‡é¢˜: {policy.title}")
else:
    print("æ”¿ç­–ä¸å­˜åœ¨")
```

---

#### get_policies()
```python
def get_policies(filters: Dict = None, 
                limit: int = 100,
                offset: int = 0) -> List[Policy]
```

æŸ¥è¯¢æ”¿ç­–åˆ—è¡¨ï¼ˆæ”¯æŒç­›é€‰å’Œåˆ†é¡µï¼‰ã€‚

**å‚æ•°**:
```python
filters = {
    "policy_type": "ç§‘æŠ€æ”¿ç­–",  # å¯é€‰
    "region": "å¹¿ä¸œ",          # å¯é€‰
    "status": "æœ‰æ•ˆ",           # å¯é€‰
    "start_date": "2024-01-01", # å¯é€‰
    "end_date": "2024-12-31",   # å¯é€‰
    "keyword": "é«˜æ–°æŠ€æœ¯"       # å¯é€‰ï¼ˆæœç´¢æ ‡é¢˜å’Œå†…å®¹ï¼‰
}
```

**è¿”å›å€¼**:
- `List[Policy]`: Policyå¯¹è±¡åˆ—è¡¨

**ç¤ºä¾‹**:
```python
# æŸ¥è¯¢å¹¿ä¸œçœçš„ç§‘æŠ€æ”¿ç­–
policies = dao.get_policies(filters={
    "policy_type": "ç§‘æŠ€æ”¿ç­–",
    "region": "å¹¿ä¸œ",
    "status": "æœ‰æ•ˆ"
}, limit=20)

for p in policies:
    print(f"{p.title} ({p.effective_date})")

# åˆ†é¡µæŸ¥è¯¢
page1 = dao.get_policies(limit=10, offset=0)
page2 = dao.get_policies(limit=10, offset=10)
```

---

#### update_policy()
```python
def update_policy(policy_id: int, metadata: Dict)
```

æ›´æ–°æ”¿ç­–è®°å½•ã€‚

**å‚æ•°**:
- `policy_id` (int): æ”¿ç­–ID
- `metadata` (Dict): è¦æ›´æ–°çš„å­—æ®µ

**ç¤ºä¾‹**:
```python
dao.update_policy(policy_id=123, metadata={
    "status": "å·²åºŸæ­¢",
    "expiry_date": "2024-06-30"
})
```

---

#### get_stats()
```python
def get_stats() -> Dict
```

è·å–æ”¿ç­–ç»Ÿè®¡ä¿¡æ¯ã€‚

**è¿”å›å€¼**:
```python
{
    "total_count": 100,
    "by_type": {
        "ç§‘æŠ€æ”¿ç­–": 30,
        "è´¢ç¨æ”¿ç­–": 25,
        "äº§ä¸šæ”¿ç­–": 20,
        # ...
    },
    "by_region": {
        "å¹¿ä¸œ": 40,
        "åŒ—äº¬": 30,
        # ...
    },
    "by_status": {
        "æœ‰æ•ˆ": 80,
        "è¿‡æœŸ": 15,
        "å³å°†è¿‡æœŸ": 5
    }
}
```

---

### GraphDAO

**æ¨¡å—**: `src/database/graph_dao.py`

#### save_graph()
```python
def save_graph(graph_data: Dict, is_incremental: bool = False)
```

ä¿å­˜çŸ¥è¯†å›¾è°±åˆ°æ•°æ®åº“ã€‚

**å‚æ•°**:
```python
graph_data = {
    "nodes": [
        {"id": "node_1", "label": "èŠ‚ç‚¹å", "type": "POLICY", ...}
    ],
    "edges": [
        {"from": "node_1", "to": "node_2", "type": "ISSUED_BY", ...}
    ]
}
```

**ç¤ºä¾‹**:
```python
from src.database.graph_dao import get_graph_dao

dao = get_graph_dao()
dao.save_graph(graph_data, is_incremental=False)
```

---

#### load_graph()
```python
def load_graph() -> Optional[Dict]
```

åŠ è½½æœ€æ–°çš„çŸ¥è¯†å›¾è°±ã€‚

**è¿”å›å€¼**:
```python
{
    "nodes": [...],
    "edges": [...],
    "metadata": {
        "node_count": 40,
        "edge_count": 73,
        "created_at": "2026-02-01T10:00:00Z"
    }
}
```

**ç¤ºä¾‹**:
```python
graph_data = dao.load_graph()
if graph_data:
    print(f"åŠ è½½å›¾è°±: {len(graph_data['nodes'])} èŠ‚ç‚¹")
else:
    print("å›¾è°±ä¸å­˜åœ¨ï¼Œè¯·å…ˆæ„å»º")
```

---

#### remove_duplicate_nodes()
```python
def remove_duplicate_nodes() -> int
```

æ¸…ç†é‡å¤èŠ‚ç‚¹ï¼ˆå»é™¤.pdfåç¼€ç­‰ï¼‰ã€‚

**è¿”å›å€¼**:
- `int`: åˆ é™¤çš„é‡å¤èŠ‚ç‚¹æ•°

**ç¤ºä¾‹**:
```python
removed = dao.remove_duplicate_nodes()
print(f"æ¸…ç†äº† {removed} ä¸ªé‡å¤èŠ‚ç‚¹")
```

---

## ğŸ’¼ ä¸šåŠ¡é€»è¾‘å±‚API

### ValidityChecker

**æ¨¡å—**: `src/business/validity_checker.py`

#### check_validity()
```python
def check_validity(policy: Policy) -> str
```

æ£€æŸ¥æ”¿ç­–æ—¶æ•ˆæ€§ã€‚

**è¿”å›å€¼**:
- `"æœ‰æ•ˆ"`: æ”¿ç­–ä»æœ‰æ•ˆ
- `"å·²è¿‡æœŸ"`: æ”¿ç­–å·²è¿‡æœŸ
- `"å³å°†è¿‡æœŸ"`: è·ç¦»è¿‡æœŸä¸åˆ°30å¤©

**ç¤ºä¾‹**:
```python
from src.business.validity_checker import ValidityChecker

checker = ValidityChecker()
status = checker.check_validity(policy)

if status == "å·²è¿‡æœŸ":
    print("è­¦å‘Š: æ”¿ç­–å·²è¿‡æœŸï¼")
```

---

### ImpactAnalyzer

**æ¨¡å—**: `src/business/impact_analyzer.py`

#### analyze_impact()
```python
def analyze_impact(policy: Policy) -> Dict
```

åˆ†ææ”¿ç­–å½±å“èŒƒå›´ã€‚

**è¿”å›å€¼**:
```python
{
    "affected_entities": ["é«˜æ–°æŠ€æœ¯ä¼ä¸š", "ç§‘æŠ€å‹ä¸­å°ä¼ä¸š"],
    "impact_scope": "å…¨çœ",
    "impact_level": "é«˜",  # é«˜/ä¸­/ä½
    "estimated_beneficiaries": 10000
}
```

---

## ğŸ”— ç›¸å…³æ–‡æ¡£

- [02-ARCHITECTURE.md](02-ARCHITECTURE.md) - ç³»ç»Ÿæ¶æ„
- [04-DEVELOPER_GUIDE.md](04-DEVELOPER_GUIDE.md) - å¼€å‘è€…æŒ‡å—
- [technical/modules-inventory.md](technical/modules-inventory.md) - æ¨¡å—æ¸…å•

---

**Last Updated**: 2026-02-01  
**Version**: 1.0
