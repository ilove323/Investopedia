"""
æ•°æ®åŒæ­¥æœåŠ¡
============
æä¾›RAGFlowæ–‡æ¡£åˆ°æœ¬åœ°æ•°æ®åº“çš„åŒæ­¥åŠŸèƒ½ï¼Œè‡ªåŠ¨æå–å…ƒæ•°æ®å¹¶ç”Ÿæˆæ ‡ç­¾ã€‚

æ ¸å¿ƒåŠŸèƒ½ï¼š
- RAGFlowæ–‡æ¡£åŒæ­¥åˆ°æœ¬åœ°æ•°æ®åº“
- è‡ªåŠ¨å…ƒæ•°æ®æå–å’Œæ ‡ç­¾ç”Ÿæˆ
- å¢é‡åŒæ­¥ï¼ˆæ›´æ–°å·²å­˜åœ¨çš„è®°å½•ï¼‰
- åŒæ­¥çŠ¶æ€å’Œé”™è¯¯æŠ¥å‘Š
- çŸ¥è¯†å›¾è°±æ„å»ºå’Œå­˜å‚¨

ä½¿ç”¨ç¤ºä¾‹ï¼š
    from src.services.data_sync import DataSyncService
    
    sync_service = DataSyncService()
    results = sync_service.sync_documents_to_database("policy_demo_kb")
    print(f"åŒæ­¥å®Œæˆ: æ–°å¢{results['new_policies']}ä¸ª")
"""
import logging
from typing import List, Dict, Any, Optional, Tuple
from datetime import datetime
from src.clients.ragflow_client import RAGFlowClient
from src.services.entity_extraction_service import get_entity_extraction_service
from src.database.policy_dao import PolicyDAO
from src.database.graph_dao import GraphDAO
from src.business.metadata_extractor import MetadataExtractor
from src.business.tag_generator import TagGenerator
from src.utils.logger import get_logger

logger = get_logger(__name__)


class DataSyncService:
    """RAGFlowåˆ°æœ¬åœ°æ•°æ®åº“çš„æ•°æ®åŒæ­¥æœåŠ¡"""
    
    def __init__(self):
        """åˆå§‹åŒ–åŒæ­¥æœåŠ¡"""
        try:
            self.ragflow = RAGFlowClient()
            self.dao = PolicyDAO()
            self.graph_dao = None  # å»¶è¿Ÿåˆå§‹åŒ–
            self.entity_service = None  # å»¶è¿Ÿåˆå§‹åŒ–
            self.metadata_extractor = MetadataExtractor()
            self.tag_generator = TagGenerator()
            logger.info("æ•°æ®åŒæ­¥æœåŠ¡åˆå§‹åŒ–å®Œæˆ")
        except Exception as e:
            logger.error(f"æ•°æ®åŒæ­¥æœåŠ¡åˆå§‹åŒ–å¤±è´¥: {e}")
            raise
    
    def _init_entity_service(self):
        """å»¶è¿Ÿåˆå§‹åŒ–å®ä½“æŠ½å–æœåŠ¡"""
        if self.entity_service is None:
            try:
                self.entity_service = get_entity_extraction_service()
                logger.info("å®ä½“æŠ½å–æœåŠ¡åˆå§‹åŒ–æˆåŠŸ")
            except Exception as e:
                logger.error(f"å®ä½“æŠ½å–æœåŠ¡åˆå§‹åŒ–å¤±è´¥: {e}")
                raise
        return self.entity_service
    
    def _init_graph_dao(self):
        """å»¶è¿Ÿåˆå§‹åŒ–GraphDAO"""
        if self.graph_dao is None:
            from src.config import get_config
            config = get_config()
            db_path = config.data_dir / "database" / "policies.db"
            self.graph_dao = GraphDAO(str(db_path))
        return self.graph_dao
    
    def sync_documents_to_database(self, kb_name: str = "policy_demo_kb") -> Dict[str, Any]:
        """
        åŒæ­¥RAGFlowæ–‡æ¡£åˆ°æœ¬åœ°æ•°æ®åº“
        
        Args:
            kb_name: çŸ¥è¯†åº“åç§°
            
        Returns:
            åŒæ­¥ç»“æœå­—å…¸ï¼ŒåŒ…å«ç»Ÿè®¡ä¿¡æ¯å’Œé”™è¯¯åˆ—è¡¨
        """
        try:
            logger.info(f"å¼€å§‹åŒæ­¥çŸ¥è¯†åº“: {kb_name}")
            
            # 1. è·å–RAGFlowä¸­çš„æ–‡æ¡£
            documents = self.ragflow.get_documents(kb_name)
            logger.info(f"ä»RAGFlowè·å–åˆ° {len(documents)} ä¸ªæ–‡æ¡£")
            
            sync_results = {
                "total_documents": len(documents),
                "new_policies": 0,
                "updated_policies": 0,
                "errors": [],
                "sync_time": datetime.now().isoformat()
            }
            
            if not documents:
                logger.warning("RAGFlowä¸­æ²¡æœ‰æ‰¾åˆ°æ–‡æ¡£")
                return sync_results
            
            for doc in documents:
                try:
                    # 2. å¤„ç†å•ä¸ªæ–‡æ¡£
                    self._sync_single_document(doc, sync_results)
                    
                except Exception as e:
                    error_msg = f"å¤„ç†æ–‡æ¡£ {doc.get('name', 'Unknown')} æ—¶å‡ºé”™: {str(e)}"
                    sync_results["errors"].append(error_msg)
                    logger.error(error_msg)
            
            logger.info(f"åŒæ­¥å®Œæˆ: æ–°å¢{sync_results['new_policies']}ä¸ª, æ›´æ–°{sync_results['updated_policies']}ä¸ª")
            return sync_results
            
        except Exception as e:
            logger.error(f"æ•°æ®åŒæ­¥å¤±è´¥: {str(e)}")
            raise
    
    def _sync_single_document(self, doc: Dict[str, Any], sync_results: Dict[str, Any]) -> None:
        """
        åŒæ­¥å•ä¸ªæ–‡æ¡£
        
        Args:
            doc: RAGFlowæ–‡æ¡£æ•°æ®
            sync_results: åŒæ­¥ç»“æœç´¯è®¡å™¨
        """
        doc_id = doc.get('id')
        doc_name = doc.get('name', 'Unknown')
        
        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
        existing_policy = self.dao.get_policy_by_ragflow_id(doc_id)
        
        # æå–å…ƒæ•°æ®
        metadata = self._extract_policy_metadata(doc)
        
        if existing_policy:
            # æ›´æ–°ç°æœ‰æ”¿ç­–
            try:
                self.dao.update_policy(existing_policy['id'], metadata)
                sync_results["updated_policies"] += 1
                logger.info(f"æ›´æ–°æ”¿ç­–: {metadata['title']}")
            except Exception as e:
                logger.warning(f"æ›´æ–°æ”¿ç­–å¤±è´¥: {metadata.get('title')}, é”™è¯¯: {e}")
                sync_results["failed_documents"] += 1
        else:
            # åˆ›å»ºæ–°æ”¿ç­–
            try:
                policy_id = self.dao.create_policy(metadata)
                
                # ç”Ÿæˆå’Œæ·»åŠ æ ‡ç­¾
                self._add_policy_tags(policy_id, metadata)
                
                sync_results["new_policies"] += 1
                logger.info(f"åˆ›å»ºæ–°æ”¿ç­–: {metadata['title']}")
            except Exception as e:
                # å¦‚æœæ–‡å·å·²å­˜åœ¨ï¼Œè®°å½•è­¦å‘Šä½†ä¸è§†ä¸ºå¤±è´¥
                if 'å·²å­˜åœ¨' in str(e) or 'UNIQUE constraint' in str(e):
                    logger.info(f"æ”¿ç­–å·²å­˜åœ¨ï¼Œè·³è¿‡: {metadata.get('title')}")
                else:
                    logger.warning(f"åˆ›å»ºæ”¿ç­–å¤±è´¥: {metadata.get('title')}, é”™è¯¯: {e}")
                    sync_results["failed_documents"] += 1
    
    def _extract_policy_metadata(self, doc: Dict[str, Any]) -> Dict[str, Any]:
        """
        ä»RAGFlowæ–‡æ¡£æå–æ”¿ç­–å…ƒæ•°æ®
        
        Args:
            doc: RAGFlowæ–‡æ¡£æ•°æ®
            
        Returns:
            æå–çš„å…ƒæ•°æ®å­—å…¸
        """
        # åŸºç¡€ä¿¡æ¯
        title = doc.get('name', '').replace('.pdf', '').replace('.docx', '')
        doc_id = doc.get('id')
        
        # ä»RAGFlowè·å–æ–‡æ¡£å®é™…å†…å®¹
        try:
            content = self.ragflow.get_document_content(doc_id) or ''
            logger.info(f"æˆåŠŸè·å–æ–‡æ¡£å†…å®¹ï¼Œé•¿åº¦: {len(content)}")
        except Exception as e:
            logger.warning(f"è·å–æ–‡æ¡£å†…å®¹å¤±è´¥: {e}")
            content = ''
        
        metadata = {
            'ragflow_document_id': doc_id,
            'title': title,
            'content': content,
            'file_path': doc.get('location', ''),
            'file_size': doc.get('size', 0),
            'upload_time': doc.get('create_time'),
            'status': 'active',
            'created_at': datetime.now().isoformat(),
            'updated_at': datetime.now().isoformat()
        }
        
        # ä½¿ç”¨å…ƒæ•°æ®æå–å™¨è¿›ä¸€æ­¥åˆ†æ
        if content:
            try:
                extracted_metadata = self.metadata_extractor.extract_all(content)
                metadata.update(extracted_metadata)
                logger.info(f"å…ƒæ•°æ®æå–å®Œæˆ: policy_type={extracted_metadata.get('policy_type')}")
            except Exception as e:
                logger.warning(f"å…ƒæ•°æ®æå–å¤±è´¥: {e}")
                # è®¾ç½®é»˜è®¤å€¼
                metadata.update({
                    'policy_type': 'unknown',
                    'issuing_authority': '',
                    'region': '',
                    'effective_date': None,
                    'document_number': ''
                })
        else:
            logger.warning(f"æ–‡æ¡£å†…å®¹ä¸ºç©ºï¼Œæ— æ³•æå–å…ƒæ•°æ®")
            metadata.update({
                'policy_type': 'unknown',
                'issuing_authority': '',
                'region': '',
                'effective_date': None,
                'document_number': ''
            })
        
        return metadata
    
    def _add_policy_tags(self, policy_id: int, metadata: Dict[str, Any]) -> None:
        """
        ä¸ºæ”¿ç­–æ·»åŠ æ ‡ç­¾
        
        Args:
            policy_id: æ”¿ç­–ID
            metadata: æ”¿ç­–å…ƒæ•°æ®
        """
        try:
            content = metadata.get('content', '')
            policy_type = metadata.get('policy_type')
            
            if not content:
                logger.warning(f"æ”¿ç­–å†…å®¹ä¸ºç©ºï¼Œè·³è¿‡æ ‡ç­¾ç”Ÿæˆ")
                return
                
            tags = self.tag_generator.generate_tags(content, policy_type=policy_type)
            for tag in tags:
                tag_name = tag.get('name')
                tag_type = tag.get('type', 'general')
                if tag_name:
                    tag_id = self.dao.get_or_create_tag(tag_name, tag_type)
                    self.dao.add_policy_tag(policy_id, tag_id)
        except Exception as e:
            logger.warning(f"æ ‡ç­¾ç”Ÿæˆå¤±è´¥: {e}")
    
    def get_sync_status(self) -> Dict[str, Any]:
        """
        è·å–åŒæ­¥çŠ¶æ€ä¿¡æ¯
        
        Returns:
            åŒ…å«æ•°æ®åº“ç»Ÿè®¡å’ŒRAGFlowè¿æ¥çŠ¶æ€çš„å­—å…¸
        """
        try:
            # æ•°æ®åº“ç»Ÿè®¡
            policies_count = len(self.dao.get_policies())
            
            # RAGFlowè¿æ¥æ£€æŸ¥
            ragflow_status = "connected"
            try:
                docs = self.ragflow.get_documents("policy_demo_kb")
                ragflow_docs_count = len(docs)
            except:
                ragflow_status = "disconnected"
                ragflow_docs_count = 0
            
            return {
                "database_policies": policies_count,
                "ragflow_documents": ragflow_docs_count,
                "ragflow_status": ragflow_status,
                "last_check": datetime.now().isoformat()
            }
        except Exception as e:
            logger.error(f"è·å–åŒæ­¥çŠ¶æ€å¤±è´¥: {e}")
            return {
                "error": str(e),
                "last_check": datetime.now().isoformat()
            }
    
    def build_knowledge_graph(self, kb_name: str = "policy_demo_kb", 
                             is_incremental: bool = False,
                             progress_callback=None,
                             max_workers: int = 3) -> Dict[str, Any]:
        """
        ä»RAGFlowæ„å»ºçŸ¥è¯†å›¾è°±å¹¶å­˜å‚¨åˆ°æ•°æ®åº“ï¼ˆæ”¯æŒå¹¶å‘å¤„ç†ï¼‰
        
        Args:
            kb_name: çŸ¥è¯†åº“åç§°
            is_incremental: æ˜¯å¦å¢é‡æ›´æ–°ï¼ˆTrue=å¢é‡ï¼ŒFalse=å…¨é‡é‡å»ºï¼‰
            progress_callback: è¿›åº¦å›è°ƒå‡½æ•°ï¼Œæ¥æ”¶(current, total, message)
            max_workers: æœ€å¤§å¹¶å‘æ•°ï¼ˆé»˜è®¤3ï¼Œé¿å…è§¦å‘APIé™æµï¼‰
            
        Returns:
            æ„å»ºç»“æœå­—å…¸
        """
        import time
        from concurrent.futures import ThreadPoolExecutor, as_completed
        
        start_time = time.time()
        
        try:
            logger.info(f"å¼€å§‹æ„å»ºçŸ¥è¯†å›¾è°± (å¢é‡={is_incremental}, å¹¶å‘æ•°={max_workers})")
            
            # åˆå§‹åŒ–GraphDAO
            graph_dao = self._init_graph_dao()
            
            # æ­¥éª¤1: è·å–æ‰€æœ‰æ–‡æ¡£
            if progress_callback:
                progress_callback(1, 5, "æ­£åœ¨è·å–æ–‡æ¡£åˆ—è¡¨...")
            
            documents = self.ragflow.get_documents(kb_name)
            if not documents:
                logger.warning("æ²¡æœ‰æ‰¾åˆ°æ–‡æ¡£ï¼Œæ— æ³•æ„å»ºå›¾è°±")
                return {
                    'success': False,
                    'error': 'çŸ¥è¯†åº“ä¸­æ²¡æœ‰æ–‡æ¡£',
                    'node_count': 0,
                    'edge_count': 0,
                    'doc_count': 0,
                    'elapsed_time': f"{time.time() - start_time:.2f}ç§’"
                }
            
            # æ­¥éª¤2: å¹¶å‘å¤„ç†æ–‡æ¡£
            if progress_callback:
                progress_callback(2, 5, f"æ­£åœ¨å¹¶å‘åˆ†æ {len(documents)} ä¸ªæ–‡æ¡£...")
            
            # åˆå§‹åŒ–å®ä½“æŠ½å–æœåŠ¡
            entity_service = self._init_entity_service()
            
            all_nodes = []
            all_edges = []
            processed_docs = 0
            seen_doc_names = set()  # ç”¨äºå»é‡æ–‡æ¡£
            seen_node_ids = set()  # ç”¨äºå»é‡èŠ‚ç‚¹ID
            failed_docs = []
            
            # å‡†å¤‡å¾…å¤„ç†æ–‡æ¡£åˆ—è¡¨ï¼ˆå»é‡ï¼‰
            docs_to_process = []
            for doc in documents:
                doc_name = doc.get('name', '').replace('.pdf', '').replace('.docx', '').strip()
                if doc_name not in seen_doc_names:
                    seen_doc_names.add(doc_name)
                    docs_to_process.append(doc)
            
            logger.info(f"å»é‡åå¾…å¤„ç†æ–‡æ¡£æ•°: {len(docs_to_process)}")
            
            # å¹¶å‘å¤„ç†æ–‡æ¡£
            with ThreadPoolExecutor(max_workers=max_workers) as executor:
                # æäº¤æ‰€æœ‰ä»»åŠ¡
                future_to_doc = {
                    executor.submit(
                        self._process_single_document,
                        doc,
                        kb_name,
                        entity_service
                    ): doc
                    for doc in docs_to_process
                }
                
                # å¤„ç†å®Œæˆçš„ä»»åŠ¡
                completed = 0
                for future in as_completed(future_to_doc):
                    completed += 1
                    doc = future_to_doc[future]
                    doc_name = doc.get('name', '')
                    
                    if progress_callback:
                        progress_callback(
                            2, 5, 
                            f"è¿›åº¦ {completed}/{len(docs_to_process)}: {doc_name[:30]}..."
                        )
                    
                    try:
                        result = future.result()
                        if result:
                            doc_nodes, doc_edges = result
                            
                            # å»é‡èŠ‚ç‚¹ï¼ˆåŸºäºIDï¼‰
                            for node in doc_nodes:
                                node_id = node.get('id')
                                if node_id and node_id not in seen_node_ids:
                                    seen_node_ids.add(node_id)
                                    all_nodes.append(node)
                            
                            all_edges.extend(doc_edges)
                            processed_docs += 1
                        else:
                            failed_docs.append(doc_name)
                    
                    except Exception as e:
                        logger.warning(f"å¤„ç†æ–‡æ¡£å¤±è´¥ {doc_name}: {e}")
                        failed_docs.append(doc_name)
                        continue
            
            if failed_docs:
                logger.warning(f"å¤±è´¥æ–‡æ¡£æ•°: {len(failed_docs)}/{len(docs_to_process)}")
            
            # æ­¥éª¤3: æ„å»ºå›¾è°±æ•°æ®ç»“æ„
            if progress_callback:
                progress_callback(3, 5, "æ­£åœ¨æ„å»ºå›¾è°±ç»“æ„...")
            
            graph_data = {
                'nodes': all_nodes,
                'edges': all_edges
            }
            
            # æ­¥éª¤4: å­˜å‚¨åˆ°æ•°æ®åº“
            if progress_callback:
                progress_callback(4, 5, "æ­£åœ¨ä¿å­˜åˆ°æ•°æ®åº“...")
            
            graph_dao.save_graph(graph_data, is_incremental=is_incremental)
            
            # æ­¥éª¤5: å®Œæˆ
            elapsed = time.time() - start_time
            if progress_callback:
                progress_callback(5, 5, "å›¾è°±æ„å»ºå®Œæˆ!")
            
            result = {
                'success': True,
                'node_count': len(all_nodes),
                'edge_count': len(all_edges),
                'doc_count': processed_docs,
                'is_incremental': is_incremental,
                'elapsed_time': f"{elapsed:.2f}ç§’",
                'timestamp': datetime.now().isoformat()
            }
            
            logger.info(f"å›¾è°±æ„å»ºæˆåŠŸ: {result['node_count']}ä¸ªèŠ‚ç‚¹, {result['edge_count']}æ¡è¾¹, è€—æ—¶{result['elapsed_time']}")
            return result
            
        except Exception as e:
            elapsed = time.time() - start_time
            logger.error(f"å›¾è°±æ„å»ºå¤±è´¥: {e}")
            return {
                'success': False,
                'error': str(e),
                'node_count': 0,
                'edge_count': 0,
                'doc_count': 0,
                'elapsed_time': f"{elapsed:.2f}ç§’"
            }
    
    def _process_single_document(self, doc: Dict, kb_name: str, entity_service) -> Optional[Tuple[List[Dict], List[Dict]]]:
        """
        å¤„ç†å•ä¸ªæ–‡æ¡£ï¼ˆç”¨äºå¹¶å‘è°ƒç”¨ï¼‰
        
        Args:
            doc: æ–‡æ¡£å¯¹è±¡
            kb_name: çŸ¥è¯†åº“åç§°
            entity_service: å®ä½“æŠ½å–æœåŠ¡
            
        Returns:
            (nodes, edges) æˆ– Noneï¼ˆå¤±è´¥æ—¶ï¼‰
        """
        try:
            doc_name = doc.get('name', '').replace('.pdf', '').replace('.docx', '').strip()
            doc_id = doc.get('id')
            
            # è·å–æ–‡æ¡£å†…å®¹
            doc_content = self.ragflow.get_document_content(doc_id, kb_name)
            
            if not doc_content or len(doc_content) < 50:
                logger.warning(f"æ–‡æ¡£å†…å®¹ä¸ºç©ºæˆ–è¿‡çŸ­: {doc_name}")
                return None
            
            # ä½¿ç”¨Qwenæå–å®ä½“å’Œå…³ç³»
            doc_nodes, doc_edges = self._extract_entities_and_relations(
                doc_content, 
                doc_name
            )
            
            return (doc_nodes, doc_edges)
            
        except Exception as e:
            logger.error(f"å¤„ç†æ–‡æ¡£å¼‚å¸¸ {doc.get('name', '')}: {e}")
            return None
    
    def _extract_entities_and_relations(self, text: str, doc_title: str) -> Tuple[List[Dict], List[Dict]]:
        """
        ä½¿ç”¨å®ä½“æŠ½å–æœåŠ¡æå–å®ä½“å’Œå…³ç³»
        
        Args:
            text: æ–‡æ¡£æ–‡æœ¬å†…å®¹
            doc_title: æ–‡æ¡£æ ‡é¢˜
            
        Returns:
            (nodes, edges) èŠ‚ç‚¹åˆ—è¡¨å’Œè¾¹åˆ—è¡¨
        """
        logger.info(f"æå–å®ä½“: {doc_title}")
        
        # è°ƒç”¨å®ä½“æŠ½å–æœåŠ¡
        result = self.entity_service.extract_from_document(text, doc_title)
        
        entities_data = result.get('entities', [])
        relations_data = result.get('relations', [])
        
        print(f"\n[DEBUG] æ–‡æ¡£: {doc_title}")
        print(f"[DEBUG] æŠ½å–ç»“æœ: {len(entities_data)}ä¸ªå®ä½“, {len(relations_data)}ä¸ªå…³ç³»")
        if entities_data:
            print(f"[DEBUG] å‰5ä¸ªå®ä½“: {[e.get('text') for e in entities_data[:5]]}")
        if relations_data:
            print(f"[DEBUG] å‰5ä¸ªå…³ç³»:")
            for i, r in enumerate(relations_data[:5], 1):
                print(f"  {i}. {r.get('source')} -> {r.get('target')} ({r.get('type')})")
        
        # æ„å»ºèŠ‚ç‚¹
        nodes = []
        entity_map = {}  # {entity_text: node_id}
        
        # é¦–å…ˆæ·»åŠ æ–‡æ¡£èŠ‚ç‚¹ï¼ˆå»æ‰.pdfç­‰åç¼€ï¼‰
        clean_doc_title = doc_title.replace('.pdf', '').replace('.docx', '').strip()
        doc_node_id = f"doc_{hash(clean_doc_title) % 100000}"
        nodes.append({
            'id': doc_node_id,
            'label': clean_doc_title,
            'type': 'document',
            'title': f'ğŸ“„ æ–‡æ¡£: {clean_doc_title}',
            'size': 30,
            'color': '#FF6B6B'
        })
        entity_map[clean_doc_title] = doc_node_id
        
        # æ·»åŠ æå–çš„å®ä½“èŠ‚ç‚¹
        for idx, entity in enumerate(entities_data):
            entity_text = entity.get('text', '').strip()
            entity_type = entity.get('type', 'unknown')
            description = entity.get('description', '')
            
            if not entity_text or len(entity_text) < 2:
                continue
            
            # é¿å…é‡å¤
            if entity_text in entity_map:
                continue
            
            node_id = f"entity_{hash(doc_title + entity_text) % 100000}"
            entity_map[entity_text] = node_id
            
            nodes.append({
                'id': node_id,
                'label': entity_text,
                'type': entity_type,
                'title': f'{self._get_entity_icon(entity_type)} {entity_type}: {entity_text}\n{description}',
                'size': self._get_entity_size(entity_type),
                'color': self._get_entity_color(entity_type)
            })
        
        # æ„å»ºè¾¹
        edges = []
        
        # æ–‡æ¡£ä¸æ‰€æœ‰å®ä½“çš„"åŒ…å«"å…³ç³»
        for entity_text, node_id in entity_map.items():
            if node_id != doc_node_id:  # æ’é™¤æ–‡æ¡£è‡ªå·±
                edges.append({
                    'from': doc_node_id,
                    'to': node_id,
                    'type': 'åŒ…å«',
                    'label': 'åŒ…å«',
                    'arrows': 'to',
                    'color': {'color': '#CCCCCC', 'opacity': 0.5}
                })
        
        print(f"\n[DEBUG] entity_mapåŒ…å« {len(entity_map)} ä¸ªå®ä½“")
        print(f"[DEBUG] entity_mapæ‰€æœ‰é”®: {list(entity_map.keys())}")
        
        # å®ä½“é—´çš„å…³ç³»
        matched_relations = 0
        for relation in relations_data:
            source_text = relation.get('source', '').strip()
            target_text = relation.get('target', '').strip()
            relation_type = relation.get('type', 'related')
            
            source_id = entity_map.get(source_text)
            target_id = entity_map.get(target_text)
            
            if not source_id:
                print(f"[WARN] å…³ç³»æºå®ä½“æœªæ‰¾åˆ°: '{source_text}'")
                continue
            
            if not target_id:
                print(f"[WARN] å…³ç³»ç›®æ ‡å®ä½“æœªæ‰¾åˆ°: '{target_text}'")
                continue
            
            if source_id and target_id and source_id != target_id:
                edges.append({
                    'from': source_id,
                    'to': target_id,
                    'type': relation_type,
                    'label': relation_type,
                    'arrows': 'to',
                    'color': {'color': self._get_relation_color(relation_type)}
                })
                matched_relations += 1
        
        print(f"\n[DEBUG] æˆåŠŸåŒ¹é… {matched_relations}/{len(relations_data)} ä¸ªå…³ç³»")
        print(f"[DEBUG] æœ€ç»ˆ: {len(nodes)}ä¸ªèŠ‚ç‚¹, {len(edges)}æ¡è¾¹")
        
        logger.info(f"å®ä½“æŠ½å–å®Œæˆ: {len(nodes)}ä¸ªèŠ‚ç‚¹, {len(edges)}æ¡è¾¹ (åŒ…å«æ–‡æ¡£å…³ç³»)")
        logger.info(f"  - å®ä½“èŠ‚ç‚¹: {len(nodes)-1}")
        logger.info(f"  - æ–‡æ¡£-å®ä½“å…³ç³»: {len([e for e in edges if e['type']=='åŒ…å«'])}")
        logger.info(f"  - å®ä½“é—´å…³ç³»: {len([e for e in edges if e['type']!='åŒ…å«'])}")
        
        return nodes, edges
    
    def _get_entity_icon(self, entity_type: str) -> str:
        """æ ¹æ®å®ä½“ç±»å‹è¿”å›emojiå›¾æ ‡"""
        icon_map = {
            'document': 'ğŸ“„',
            'æ”¿ç­–åç§°': 'ğŸ“‹',
            'æ³•å¾‹æ³•è§„': 'âš–ï¸',
            'å‘æ–‡æœºå…³': 'ğŸ›ï¸',
            'åœ°åŒº': 'ğŸŒ',
            'é¢†åŸŸ': 'ğŸ¯',
            'æ–‡å·': 'ğŸ”–',
            'æ—¶é—´': 'ğŸ“…',
            'å…³é”®æ¦‚å¿µ': 'ğŸ’¡',
        }
        return icon_map.get(entity_type, 'ğŸ”¹')
    
    def _get_entity_size(self, entity_type: str) -> int:
        """æ ¹æ®å®ä½“ç±»å‹è¿”å›èŠ‚ç‚¹å¤§å°"""
        size_map = {
            'document': 30,
            'æ”¿ç­–åç§°': 25,
            'æ³•å¾‹æ³•è§„': 25,
            'å‘æ–‡æœºå…³': 20,
            'åœ°åŒº': 18,
            'é¢†åŸŸ': 18,
            'æ–‡å·': 15,
            'æ—¶é—´': 12,
            'å…³é”®æ¦‚å¿µ': 15,
        }
        return size_map.get(entity_type, 15)
    
    def _get_entity_color(self, entity_type: str) -> str:
        """æ ¹æ®å®ä½“ç±»å‹è¿”å›èŠ‚ç‚¹é¢œè‰²"""
        color_map = {
            'document': '#FF6B6B',
            'æ”¿ç­–åç§°': '#4ECDC4',
            'æ³•å¾‹æ³•è§„': '#45B7D1',
            'å‘æ–‡æœºå…³': '#FFA07A',
            'åœ°åŒº': '#F7DC6F',
            'é¢†åŸŸ': '#BB8FCE',
            'æ–‡å·': '#98D8C8',
            'æ—¶é—´': '#85C1E2',
            'å…³é”®æ¦‚å¿µ': '#52BE80',
        }
        return color_map.get(entity_type, '#95A5A6')
    
    def _get_relation_color(self, relation_type: str) -> str:
        """æ ¹æ®å…³ç³»ç±»å‹è¿”å›è¾¹é¢œè‰²"""
        color_map = {
            'å‘å¸ƒ': '#FF6B6B',
            'ä¾æ®': '#4ECDC4',
            'é€‚ç”¨äº': '#F7DC6F',
            'æ¶‰åŠ': '#BB8FCE',
            'ä¿®è®¢': '#FFA07A',
            'åºŸæ­¢': '#E74C3C',
            'å¼•ç”¨': '#98D8C8',
            'å®æ–½æ—¶é—´': '#85C1E2',
            'åŒ…å«': '#CCCCCC',
        }
        return color_map.get(relation_type, '#95A5A6')


def get_data_sync_service() -> DataSyncService:
    """è·å–æ•°æ®åŒæ­¥æœåŠ¡å®ä¾‹"""
    return DataSyncService()