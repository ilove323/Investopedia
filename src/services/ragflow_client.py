"""
RAGFlowæœåŠ¡å®¢æˆ·ç«¯
=================
è´Ÿè´£ä¸RAGFlowæœåŠ¡çš„APIäº¤äº’ã€‚

æ ¸å¿ƒåŠŸèƒ½ï¼š
- å¥åº·æ£€æŸ¥ï¼ˆéªŒè¯æœåŠ¡è¿æ¥ï¼‰
- æ–‡æ¡£ä¸Šä¼ ï¼ˆå‘RAGFlowä¸Šä¼ æ”¿ç­–æ–‡æ¡£ï¼‰
- æ–‡æ¡£åˆ é™¤ï¼ˆä»RAGFlowåˆ é™¤æ–‡æ¡£ï¼‰
- è¯­ä¹‰æœç´¢ï¼ˆåŸºäºå‘é‡ç›¸ä¼¼åº¦æœç´¢ï¼‰
- é—®ç­”åŠŸèƒ½ï¼ˆè°ƒç”¨RAGFlowçš„é—®ç­”APIï¼‰
- çŸ¥è¯†åº“é…ç½®ç®¡ç†

æœ¬æ¨¡å—ä½¿ç”¨å®˜æ–¹RAGFlow SDKå®ç°æ‰€æœ‰åŠŸèƒ½ï¼Œä¸å†ä½¿ç”¨è‡ªå®šä¹‰HTTPå®¢æˆ·ç«¯ã€‚

ä¾èµ–ï¼š
- ragflow_sdk - RAGFlowå®˜æ–¹Python SDK
- src.config.ConfigLoader - RAGFlowæœåŠ¡é…ç½®

é…ç½®é¡¹ï¼ˆæ¥è‡ªconfig.iniçš„[RAGFLOW]éƒ¨åˆ†ï¼‰ï¼š
- host: RAGFlowæœåŠ¡ä¸»æœºåœ°å€
- port: RAGFlowæœåŠ¡ç«¯å£
- api_key: RAGFlow APIå¯†é’¥

ä½¿ç”¨ç¤ºä¾‹ï¼š
    from src.services.ragflow_client import get_ragflow_client

    client = get_ragflow_client()

    # æ£€æŸ¥è¿æ¥
    if client.check_health():
        print("RAGFlowæœåŠ¡æ­£å¸¸")

    # ä¸Šä¼ æ–‡æ¡£
    doc_id = client.upload_document("policy.pdf", "policy.pdf")

    # æœç´¢
    results = client.search("æ”¿ç­–å†…å®¹", top_k=10)
"""
import logging
from typing import Optional, Dict, List, Any

# ===== å¯¼å…¥å®˜æ–¹RAGFlow SDK =====
try:
    from ragflow_sdk import RAGFlow
except ImportError:
    RAGFlow = None
    print("Warning: ragflow-sdk not installed. Please run: pip install ragflow-sdk")

# ===== å¯¼å…¥é…ç½®ç³»ç»Ÿ =====
from src.config import get_config

# ===== è·å–RAGFlowé…ç½® =====
config = get_config()

RAGFLOW_BASE_URL = config.ragflow_base_url  # RAGFlowæœåŠ¡URLï¼ˆhttp://host:portï¼‰
RAGFLOW_API_KEY = config.ragflow_api_key  # RAGFlow APIå¯†é’¥ï¼ˆå¦‚æœéœ€è¦è®¤è¯ï¼‰
RAGFLOW_TIMEOUT = config.ragflow_timeout  # APIè°ƒç”¨è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
RAGFLOW_RETRY_TIMES = config.ragflow_retry_times  # å¤±è´¥é‡è¯•æ¬¡æ•°
RAGFLOW_RETRY_DELAY = config.ragflow_retry_delay  # é‡è¯•å»¶è¿Ÿï¼ˆç§’ï¼‰
RAGFLOW_SEARCH_CONFIG = config.ragflow_search_config  # æœç´¢é…ç½®ï¼ˆtop_k, thresholdç­‰ï¼‰
RAGFLOW_QA_CONFIG = config.ragflow_qa_config  # é—®ç­”é…ç½®ï¼ˆmax_tokens, temperatureç­‰ï¼‰

logger = logging.getLogger(__name__)


class RAGFlowClient:
    """RAGFlowå®¢æˆ·ç«¯ - ä½¿ç”¨å®˜æ–¹SDK"""

    def __init__(self, auto_configure: bool = True):
        """åˆå§‹åŒ–RAGFlowå®¢æˆ·ç«¯

        Args:
            auto_configure: æ˜¯å¦åœ¨åˆå§‹åŒ–æ—¶è‡ªåŠ¨åº”ç”¨é…ç½®å‚æ•°
        """
        if RAGFlow is None:
            raise ImportError("RAGFlow SDK not available. Please install: pip install ragflow-sdk")

        # åˆå§‹åŒ–å®˜æ–¹SDKå®¢æˆ·ç«¯
        self.rag = RAGFlow(
            api_key=RAGFLOW_API_KEY,
            base_url=RAGFLOW_BASE_URL
        )

        # å­˜å‚¨çŸ¥è¯†åº“å’ŒèŠå¤©åŠ©æ‰‹çš„ç¼“å­˜
        self._dataset_cache = {}
        self._chat_cache = {}

        logger.info(f"RAGFlow SDK initialized: {RAGFLOW_BASE_URL}")

        # è‡ªåŠ¨åº”ç”¨é…ç½®å‚æ•°
        if auto_configure:
            self._apply_configuration()

    def _apply_configuration(self):
        """åº”ç”¨é…ç½®æ–‡ä»¶ä¸­çš„RAGFlowå‚æ•°"""
        try:
            from src.config import get_config

            config = get_config()

            # è·å–é»˜è®¤çŸ¥è¯†åº“é…ç½®
            kb_name = config.default_kb_name
            kb_config = config.get_kb_config(kb_name)

            if not kb_config:
                logger.warning(f"âš ï¸ æ— æ³•åŠ è½½çŸ¥è¯†åº“ '{kb_name}' çš„é…ç½®")
                return

            logger.info(f"å¼€å§‹åº”ç”¨çŸ¥è¯†åº“ '{kb_name}' çš„é…ç½®...")

            # 1. é¦–å…ˆæ£€æŸ¥çŸ¥è¯†åº“æ˜¯å¦å­˜åœ¨
            if not self._check_knowledge_base_exists(kb_config['kb_name']):
                logger.warning(f"âš ï¸ çŸ¥è¯†åº“ '{kb_config['kb_name']}' ä¸å­˜åœ¨")
                logger.info(f"ğŸ’¡ è¯·åœ¨RAGFlow Webç•Œé¢ ({RAGFLOW_BASE_URL}) ä¸­åˆ›å»ºçŸ¥è¯†åº“")
                logger.warning("RAGFlowé…ç½®å¯èƒ½æœªå®Œå…¨ç”Ÿæ•ˆ")
                return

            logger.info(f"ğŸ“‹ åº”ç”¨é…ç½®: {len(kb_config)} ä¸ªå‚æ•°")

            # 2. åº”ç”¨çŸ¥è¯†åº“é…ç½®
            success = self._update_knowledge_base_config(kb_config['kb_name'], kb_config)

            if success:
                logger.info("âœ… çŸ¥è¯†åº“é…ç½®åº”ç”¨æˆåŠŸ")
                logger.info(f"ğŸ›ï¸ é…ç½®è¯¦æƒ…: åˆ†å—å¤§å°={kb_config.get('chunk_size')}, "
                          f"ç›¸ä¼¼åº¦é˜ˆå€¼={kb_config.get('similarity_threshold')}, "
                          f"å›¾è°±æ£€ç´¢={kb_config.get('graph_retrieval')}")
            else:
                logger.warning("âš ï¸ çŸ¥è¯†åº“é…ç½®å¯èƒ½æœªå®Œå…¨ç”Ÿæ•ˆ")

        except Exception as e:
            logger.warning(f"è‡ªåŠ¨é…ç½®å¤±è´¥: {e}")
            logger.warning("RAGFlowé…ç½®å¯èƒ½æœªå®Œå…¨ç”Ÿæ•ˆ")

    def _get_or_create_dataset(self, kb_name: str):
        """è·å–æˆ–ç¼“å­˜æ•°æ®é›†å¯¹è±¡

        Args:
            kb_name: çŸ¥è¯†åº“åç§°

        Returns:
            æ•°æ®é›†å¯¹è±¡ï¼Œå¦‚æœæœªæ‰¾åˆ°è¿”å›None
        """
        # æ£€æŸ¥ç¼“å­˜
        if kb_name in self._dataset_cache:
            return self._dataset_cache[kb_name]

        try:
            # ä½¿ç”¨SDKåˆ—å‡ºæ•°æ®é›†
            datasets = self.rag.list_datasets(name=kb_name)
            if datasets:
                dataset = datasets[0]
                self._dataset_cache[kb_name] = dataset
                logger.debug(f"Dataset cached: {kb_name} (ID: {dataset.id})")
                return dataset

            logger.error(f"Dataset '{kb_name}' not found")
            return None
        except Exception as e:
            logger.error(f"Failed to get dataset '{kb_name}': {e}")
            return None

    def _check_knowledge_base_exists(self, kb_name: str) -> bool:
        """
        æ£€æŸ¥çŸ¥è¯†åº“æ˜¯å¦å­˜åœ¨

        Args:
            kb_name: çŸ¥è¯†åº“åç§°

        Returns:
            bool: çŸ¥è¯†åº“æ˜¯å¦å­˜åœ¨
        """
        try:
            datasets = self.rag.list_datasets(name=kb_name)
            if datasets:
                logger.info(f"âœ… çŸ¥è¯†åº“ '{kb_name}' å­˜åœ¨")
                return True
            logger.warning(f"âŒ çŸ¥è¯†åº“ '{kb_name}' ä¸å­˜åœ¨")
            return False
        except Exception as e:
            logger.warning(f"çŸ¥è¯†åº“å­˜åœ¨æ€§æ£€æŸ¥å¤±è´¥: {e}")
            return False

    def _update_knowledge_base_config(self, kb_name: str, config_params: dict) -> bool:
        """æ›´æ–°çŸ¥è¯†åº“é…ç½®

        Args:
            kb_name: çŸ¥è¯†åº“åç§°
            config_params: é…ç½®å‚æ•°å­—å…¸

        Returns:
            æ›´æ–°æ˜¯å¦æˆåŠŸ
        """
        try:
            logger.debug(f"å¼€å§‹æ›´æ–°çŸ¥è¯†åº“ '{kb_name}' çš„é…ç½®...")

            # è·å–æ•°æ®é›†å¯¹è±¡
            dataset = self._get_or_create_dataset(kb_name)
            if not dataset:
                logger.error(f"æ— æ³•è·å–çŸ¥è¯†åº“ '{kb_name}'")
                return False

            # æ„å»ºç¬¦åˆAPIæ–‡æ¡£çš„é…ç½®æ•°æ®
            update_data = self._build_dataset_update_payload(config_params)

            logger.debug(f"æ›´æ–°æ•°æ®: {update_data}")
            logger.info(f"å‘çŸ¥è¯†åº“ '{kb_name}' åº”ç”¨é…ç½®...")

            # ä½¿ç”¨SDKæ›´æ–°æ•°æ®é›†é…ç½®
            dataset.update(update_data)

            logger.info(f"âœ… çŸ¥è¯†åº“é…ç½®æ›´æ–°æˆåŠŸ: {kb_name}")
            return True

        except Exception as e:
            logger.error(f"çŸ¥è¯†åº“é…ç½®æ›´æ–°å¼‚å¸¸: {e}")
            return False

    def _get_knowledge_base_id(self, kb_name: str) -> Optional[str]:
        """è·å–çŸ¥è¯†åº“ID

        Args:
            kb_name: çŸ¥è¯†åº“åç§°

        Returns:
            çŸ¥è¯†åº“IDï¼Œå¦‚æœæœªæ‰¾åˆ°è¿”å›None
        """
        try:
            dataset = self._get_or_create_dataset(kb_name)
            return dataset.id if dataset else None
        except Exception as e:
            logger.error(f"è·å–çŸ¥è¯†åº“IDå¤±è´¥: {e}")
            return None

    def _get_or_create_chat_assistant(self, knowledge_base_name: str):
        """è·å–æˆ–åˆ›å»ºèŠå¤©åŠ©æ‰‹

        Args:
            knowledge_base_name: çŸ¥è¯†åº“åç§°

        Returns:
            èŠå¤©åŠ©æ‰‹å¯¹è±¡
        """
        try:
            # æ£€æŸ¥ç¼“å­˜
            if knowledge_base_name in self._chat_cache:
                return self._chat_cache[knowledge_base_name]

            # è·å–çŸ¥è¯†åº“
            datasets = self.rag.list_datasets(name=knowledge_base_name)
            if not datasets:
                logger.error(f"çŸ¥è¯†åº“ '{knowledge_base_name}' ä¸å­˜åœ¨")
                return None

            dataset = datasets[0]
            dataset_id = dataset.id

            # è·å–ç³»ç»Ÿæç¤ºè¯é…ç½®
            from src.config import get_config
            config = get_config()
            kb_config = config.get_kb_config(knowledge_base_name)
            system_prompt = ""

            if kb_config and 'system_prompt' in kb_config:
                system_prompt = kb_config['system_prompt']
                logger.info(f"ä½¿ç”¨è‡ªå®šä¹‰ç³»ç»Ÿæç¤ºè¯ (é•¿åº¦: {len(system_prompt)})")

            # æŸ¥æ‰¾ç°æœ‰çš„èŠå¤©åŠ©æ‰‹
            chat_name = f"PolicyChat_{knowledge_base_name}"
            existing_chats = self.rag.list_chats(name=chat_name)

            if existing_chats:
                chat_assistant = existing_chats[0]
                logger.info(f"æ‰¾åˆ°ç°æœ‰èŠå¤©åŠ©æ‰‹: {chat_name}")

                # å¦‚æœæœ‰æ–°çš„ç³»ç»Ÿæç¤ºè¯ï¼Œæ›´æ–°èŠå¤©åŠ©æ‰‹
                if system_prompt and system_prompt.strip():
                    chat_assistant.update({
                        "prompt": {
                            "prompt": system_prompt,
                            "top_n": 8,
                            "similarity_threshold": 0.2
                        }
                    })
                    logger.info("æ›´æ–°èŠå¤©åŠ©æ‰‹çš„ç³»ç»Ÿæç¤ºè¯")
            else:
                # åˆ›å»ºæ–°çš„èŠå¤©åŠ©æ‰‹
                logger.info(f"åˆ›å»ºæ–°èŠå¤©åŠ©æ‰‹: {chat_name}")

                # æ„å»ºæç¤ºè¯é…ç½®
                prompt_config = None
                if system_prompt and system_prompt.strip():
                    from ragflow_sdk import Chat
                    prompt_config = Chat.Prompt(
                        prompt=system_prompt,
                        top_n=8,
                        similarity_threshold=0.2,
                        keywords_similarity_weight=0.7
                    )

                chat_assistant = self.rag.create_chat(
                    name=chat_name,
                    dataset_ids=[dataset_id],
                    prompt=prompt_config
                )
                logger.info(f"èŠå¤©åŠ©æ‰‹åˆ›å»ºæˆåŠŸ: {chat_assistant.id}")

            # ç¼“å­˜èŠå¤©åŠ©æ‰‹
            self._chat_cache[knowledge_base_name] = chat_assistant
            return chat_assistant

        except Exception as e:
            logger.error(f"è·å–æˆ–åˆ›å»ºèŠå¤©åŠ©æ‰‹å¤±è´¥: {e}")
            return None

    def _get_or_create_session(self, chat_assistant):
        """è·å–æˆ–åˆ›å»ºä¼šè¯

        Args:
            chat_assistant: èŠå¤©åŠ©æ‰‹å¯¹è±¡

        Returns:
            ä¼šè¯å¯¹è±¡
        """
        try:
            # è·å–ç°æœ‰ä¼šè¯
            sessions = chat_assistant.list_sessions(page_size=1)

            if sessions:
                session = sessions[0]
                logger.debug(f"ä½¿ç”¨ç°æœ‰ä¼šè¯: {session.id}")
            else:
                # åˆ›å»ºæ–°ä¼šè¯
                session = chat_assistant.create_session("Policy Assistant Session")
                logger.debug(f"åˆ›å»ºæ–°ä¼šè¯: {session.id}")

            return session

        except Exception as e:
            logger.error(f"è·å–æˆ–åˆ›å»ºä¼šè¯å¤±è´¥: {e}")
            return None

    def _build_dataset_update_payload(self, config_params: dict) -> dict:
        """æ ¹æ®APIæ–‡æ¡£æ„å»ºæ•°æ®é›†æ›´æ–°è½½è·

        Args:
            config_params: é…ç½®å‚æ•°

        Returns:
            ç¬¦åˆAPIæ–‡æ¡£æ ¼å¼çš„æ›´æ–°æ•°æ®
        """
        # åŸºç¡€æ›´æ–°æ•°æ®
        update_data = {}

        # Note: System prompts should be set via Chat Assistant, not Dataset
        # Dataset.update() doesn't accept prompt/system_prompt/llm_setting fields
        # These are configured when creating/updating chat assistants instead

        # è®¾ç½®åˆ†å—æ–¹æ³•
        chunk_method = "naive"  # é»˜è®¤ä½¿ç”¨Generalæ–¹æ³•
        if config_params.get("pdf_parser") == "deepdoc":
            chunk_method = "naive"  # deepdocå¯¹åº”General
        elif config_params.get("pdf_parser") == "laws":
            chunk_method = "laws"

        update_data["chunk_method"] = chunk_method

        # æ„å»ºparser_configæ ¹æ®chunk_method
        parser_config = {}

        if chunk_method == "naive":
            # Generalæ–¹æ³•çš„parser_config
            parser_config = {
                "chunk_token_num": config_params.get("chunk_size", 800),
                "auto_keywords": 1 if config_params.get("auto_keywords", True) else 0,
                "auto_questions": 0,  # ä¸å¯ç”¨è‡ªåŠ¨é—®é¢˜ç”Ÿæˆ
                "delimiter": "\\n",
                "html4excel": False,
                "layout_recognize": "deepdoc",  # ä½¿ç”¨deepdocå¸ƒå±€è¯†åˆ«
                "task_page_size": 12,
                "raptor": {
                    "use_raptor": config_params.get("graph_retrieval", True),
                    "max_cluster": config_params.get("max_clusters", 50),
                    "max_token": config_params.get("max_tokens", 256),
                    "threshold": config_params.get("similarity_threshold", 0.3),
                    "random_seed": config_params.get("random_seed", 42)
                },
                "graphrag": {
                    "use_graphrag": config_params.get("graph_retrieval", True),
                    "entity_types": ["organization", "person", "geo", "event", "category"],
                    "method": config_params.get("retrieval_mode", "general"),
                    "resolution": config_params.get("entity_normalization", True)
                }
            }

        elif chunk_method == "laws":
            # Lawsæ–¹æ³•çš„parser_config (åªæœ‰raptoré…ç½®)
            parser_config = {
                "raptor": {
                    "use_raptor": config_params.get("graph_retrieval", True),
                    "max_cluster": config_params.get("max_clusters", 50),
                    "max_token": config_params.get("max_tokens", 256),
                    "threshold": config_params.get("similarity_threshold", 0.3),
                    "random_seed": config_params.get("random_seed", 42)
                }
            }

        update_data["parser_config"] = parser_config

        return update_data

    def check_health(self) -> bool:
        """
        æ£€æŸ¥RAGFlowæœåŠ¡å¥åº·çŠ¶æ€

        é€šè¿‡å°è¯•åˆ—å‡ºæ•°æ®é›†æ¥éªŒè¯SDKè¿æ¥
        """
        try:
            # å°è¯•åˆ—å‡ºæ•°æ®é›†ä»¥éªŒè¯è¿æ¥
            self.rag.list_datasets(page=1, page_size=1)
            return True
        except Exception as e:
            logger.debug(f"RAGFlow health check failed: {e}")
            return False

    def upload_document(self, file_path: str, file_name: str,
                       knowledge_base_name: Optional[str] = None) -> Optional[str]:
        """
        ä¸Šä¼ æ–‡æ¡£åˆ°RAGFlow

        Args:
            file_path: æœ¬åœ°æ–‡ä»¶è·¯å¾„
            file_name: æ–‡ä»¶å
            knowledge_base_name: çŸ¥è¯†åº“åç§°ï¼ˆå¦‚ä¸æŒ‡å®šåˆ™ä»config.iniè¯»å–ï¼‰

        Returns:
            æ–‡æ¡£IDï¼Œå¤±è´¥è¿”å›None
        """
        try:
            # å¦‚æœæœªæŒ‡å®šçŸ¥è¯†åº“åç§°ï¼Œä»é…ç½®è¯»å–
            if knowledge_base_name is None:
                knowledge_base_name = config.default_kb_name

            # è·å–æ•°æ®é›†
            dataset = self._get_or_create_dataset(knowledge_base_name)
            if not dataset:
                logger.error(f"Knowledge base '{knowledge_base_name}' not found")
                return None

            # è¯»å–æ–‡ä»¶å†…å®¹
            with open(file_path, 'rb') as f:
                file_content = f.read()

            logger.info(f"ä¸Šä¼ æ–‡æ¡£åˆ°çŸ¥è¯†åº“ '{knowledge_base_name}': {file_name}")

            # ä½¿ç”¨SDKä¸Šä¼ æ–‡æ¡£
            documents = dataset.upload_documents([{
                "display_name": file_name,
                "blob": file_content
            }])

            if documents:
                doc_id = documents[0].id
                logger.info(f"âœ… æ–‡æ¡£ä¸Šä¼ æˆåŠŸ: {file_name} -> {doc_id}")
                return doc_id

            logger.warning(f"æ–‡æ¡£ä¸Šä¼ æœªè¿”å›æ–‡æ¡£ID")
            return None

        except FileNotFoundError:
            logger.error(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
            return None
        except Exception as e:
            logger.error(f"ä¸Šä¼ æ–‡æ¡£å¤±è´¥: {type(e).__name__}: {e}")
            return None

    def delete_document(self, doc_id: str, kb_name: Optional[str] = None) -> bool:
        """
        åˆ é™¤RAGFlowä¸­çš„æ–‡æ¡£

        Args:
            doc_id: æ–‡æ¡£ID
            kb_name: çŸ¥è¯†åº“åç§°ï¼ˆå¯é€‰ï¼‰

        Returns:
            Trueè¡¨ç¤ºåˆ é™¤æˆåŠŸ
        """
        try:
            kb_name = kb_name or config.default_kb_name
            dataset = self._get_or_create_dataset(kb_name)
            if not dataset:
                return False

            # ä½¿ç”¨SDKåˆ é™¤æ–‡æ¡£
            dataset.delete_documents(ids=[doc_id])
            logger.info(f"æ–‡æ¡£åˆ é™¤æˆåŠŸ: {doc_id}")
            return True
        except Exception as e:
            logger.error(f"åˆ é™¤æ–‡æ¡£å¤±è´¥: {e}")
            return False

    def search(self, query: str, knowledge_base_name: str = "policy_demo_kb",
               top_k: Optional[int] = None, score_threshold: Optional[float] = None) -> List[Dict[str, Any]]:
        """
        åœ¨RAGFlowä¸­è¿›è¡Œè¯­ä¹‰æœç´¢

        Args:
            query: æœç´¢æŸ¥è¯¢
            knowledge_base_name: çŸ¥è¯†åº“åç§°
            top_k: è¿”å›ç»“æœæ•°
            score_threshold: ç›¸ä¼¼åº¦é˜ˆå€¼

        Returns:
            æœç´¢ç»“æœåˆ—è¡¨
        """
        try:
            top_k = top_k or RAGFLOW_SEARCH_CONFIG['top_k']

            dataset = self._get_or_create_dataset(knowledge_base_name)
            if not dataset:
                return []

            # ä½¿ç”¨SDKæ£€ç´¢æ–¹æ³•
            chunks = dataset.retrieve(
                question=query,
                limit=top_k
            )

            # è½¬æ¢ä¸ºé¢„æœŸæ ¼å¼
            results = []
            for chunk in chunks:
                results.append({
                    'content': chunk.content,
                    'document_name': getattr(chunk, 'document_name', ''),
                    'similarity': getattr(chunk, 'similarity', 0.0),
                    'chunk_id': chunk.id
                })

            logger.info(f"æœç´¢å®Œæˆ: '{query}' è¿”å› {len(results)} ç»“æœ")
            return results

        except Exception as e:
            logger.error(f"æœç´¢å¤±è´¥: {e}")
            return []

    def ask(self, query: str, knowledge_base_name: str = "policy_demo_kb",
            context_limit: int = 5) -> Optional[Dict[str, Any]]:
        """
        åœ¨RAGFlowä¸­è¿›è¡Œé—®ç­”

        Args:
            query: é—®é¢˜
            knowledge_base_name: çŸ¥è¯†åº“åç§°
            context_limit: ä¸Šä¸‹æ–‡é™åˆ¶

        Returns:
            é—®ç­”ç»“æœ
        """
        try:
            # è·å–æˆ–åˆ›å»ºèŠå¤©åŠ©æ‰‹
            chat_assistant = self._get_or_create_chat_assistant(knowledge_base_name)
            if not chat_assistant:
                logger.error(f"æ— æ³•è·å–çŸ¥è¯†åº“ '{knowledge_base_name}' çš„èŠå¤©åŠ©æ‰‹")
                return None

            # è·å–æˆ–åˆ›å»ºä¼šè¯
            session = self._get_or_create_session(chat_assistant)
            if not session:
                logger.error(f"æ— æ³•åˆ›å»ºèŠå¤©ä¼šè¯")
                return None

            # è¿›è¡Œé—®ç­”
            logger.debug(f"å‘èŠå¤©åŠ©æ‰‹æé—®: {query}")
            message = session.ask(question=query, stream=False)

            if message:
                result = {
                    'answer': message.content,
                    'message_id': message.id,
                    'references': []
                }

                # å¤„ç†å¼•ç”¨æ–‡æ¡£
                if hasattr(message, 'reference') and message.reference:
                    for ref in message.reference:
                        if hasattr(ref, 'content'):
                            result['references'].append({
                                'content': ref.content,
                                'document_name': getattr(ref, 'document_name', ''),
                                'similarity': getattr(ref, 'similarity', 0.0)
                            })

                logger.info(f"é—®ç­”å®Œæˆ: '{query}' -> {len(result['answer'])} å­—ç¬¦")
                return result
            else:
                logger.error("èŠå¤©åŠ©æ‰‹è¿”å›ç©ºå“åº”")
                return None

        except Exception as e:
            logger.error(f"é—®ç­”å¼‚å¸¸: {e}")
            return None

    def get_documents(self, knowledge_base_name: str = "policy_demo_kb") -> List[Dict[str, Any]]:
        """
        è·å–çŸ¥è¯†åº“ä¸­çš„æ–‡æ¡£åˆ—è¡¨

        Args:
            knowledge_base_name: çŸ¥è¯†åº“åç§°

        Returns:
            æ–‡æ¡£åˆ—è¡¨ï¼ŒåŒ…å«å®Œæ•´çš„æ–‡æ¡£å…ƒæ•°æ®
        """
        try:
            dataset = self._get_or_create_dataset(knowledge_base_name)
            if not dataset:
                return []

            # ä½¿ç”¨SDKåˆ—å‡ºæ–‡æ¡£
            docs = dataset.list_documents(page=1, page_size=100)

            # è½¬æ¢ä¸ºé¢„æœŸæ ¼å¼ï¼Œæå–æ‰€æœ‰å¯ç”¨å­—æ®µ
            documents = []
            for doc in docs:
                doc_info = {
                    'id': doc.id,
                    'name': doc.name,
                    'size': getattr(doc, 'size', 0),
                    'status': getattr(doc, 'status', ''),
                    'create_time': getattr(doc, 'create_time', ''),
                    'update_time': getattr(doc, 'update_time', ''),
                    # åˆ†å—å’Œtokenä¿¡æ¯
                    'chunk_num': getattr(doc, 'chunk_num', 0),
                    'token_num': getattr(doc, 'token_num', 0),
                    # è§£æå™¨ä¿¡æ¯
                    'parser_id': getattr(doc, 'parser_id', ''),
                    'parser_config': getattr(doc, 'parser_config', {}),
                    # å¤„ç†è¿›åº¦
                    'progress': getattr(doc, 'progress', 0),
                    'progress_msg': getattr(doc, 'progress_msg', ''),
                    # å…¶ä»–å…ƒæ•°æ®
                    'type': getattr(doc, 'type', ''),
                    'location': getattr(doc, 'location', ''),
                }
                documents.append(doc_info)

            logger.info(f"è·å–æ–‡æ¡£åˆ—è¡¨æˆåŠŸ: {len(documents)} ä¸ªæ–‡æ¡£")
            return documents

        except Exception as e:
            logger.error(f"è·å–æ–‡æ¡£åˆ—è¡¨å¤±è´¥: {e}")
            return []

    def get_document_content(self, doc_id: str, kb_name: Optional[str] = None) -> Optional[str]:
        """
        è·å–æ–‡æ¡£çš„å®Œæ•´å†…å®¹

        Args:
            doc_id: æ–‡æ¡£ID
            kb_name: çŸ¥è¯†åº“åç§°

        Returns:
            æ–‡æ¡£å†…å®¹ï¼Œå¤±è´¥è¿”å›None
        """
        try:
            kb_name = kb_name or config.default_kb_name
            dataset = self._get_or_create_dataset(kb_name)
            if not dataset:
                return None

            # è·å–æ–‡æ¡£
            docs = dataset.list_documents(id=doc_id)
            if not docs:
                logger.warning(f"Document not found: {doc_id}")
                return None

            doc = docs[0]
            doc_name = getattr(doc, 'name', '') or ''

            # å°è¯•ä¸‹è½½æ–‡æ¡£å†…å®¹å¹¶è§£æ
            try:
                content_bytes = doc.download()
                if content_bytes:
                    # æ ¹æ®æ–‡ä»¶ç±»å‹è¿›è¡Œä¸åŒçš„å¤„ç†
                    if doc_name.lower().endswith('.pdf'):
                        # PDFæ–‡ä»¶å¤„ç†
                        return self._extract_pdf_content(content_bytes, doc_name)
                    elif doc_name.lower().endswith(('.txt', '.md', '.json', '.xml', '.csv')):
                        # æ–‡æœ¬æ–‡ä»¶å¤„ç†
                        return self._extract_text_content(content_bytes)
                    else:
                        # å…¶ä»–æ–‡ä»¶ç±»å‹ï¼Œå°è¯•æ–‡æœ¬æå–
                        return self._extract_text_content(content_bytes)
            except Exception as e:
                logger.warning(f"æ–‡æ¡£ä¸‹è½½æˆ–è§£æå¤±è´¥ (doc_id: {doc_id}): {e}")

            # å›é€€ï¼šèšåˆå—å†…å®¹
            chunks = doc.list_chunks()
            if chunks:
                content = "\n\n".join([chunk.content for chunk in chunks])
                logger.info(f"Retrieved document content from chunks: {doc_id}")
                return content

            return None
        except Exception as e:
            logger.error(f"è·å–æ–‡æ¡£å†…å®¹å¤±è´¥ (doc_id: {doc_id}): {e}")
            return None

    def _extract_pdf_content(self, content_bytes: bytes, doc_name: str) -> str:
        """
        ä»PDFå­—èŠ‚å†…å®¹æå–æ–‡æœ¬
        
        Args:
            content_bytes: PDFæ–‡ä»¶çš„äºŒè¿›åˆ¶å†…å®¹
            doc_name: æ–‡æ¡£åç§°ï¼ˆç”¨äºæ—¥å¿—ï¼‰
            
        Returns:
            æå–çš„æ–‡æœ¬å†…å®¹
        """
        import warnings
        import logging
        
        # æŠ‘åˆ¶PDFè§£æè­¦å‘Š
        warnings.filterwarnings('ignore', message='.*FontBBox.*')
        warnings.filterwarnings('ignore', message='.*cannot be parsed.*')
        
        # æŠ‘åˆ¶PDFåº“çš„æ—¥å¿—è¾“å‡º
        logging.getLogger('pdfplumber').setLevel(logging.ERROR)
        logging.getLogger('pdfminer').setLevel(logging.ERROR)
        logging.getLogger('pdfminer.layout').setLevel(logging.ERROR)
        logging.getLogger('pdfminer.converter').setLevel(logging.ERROR)
        
        try:
            import io
            
            # å°è¯•ä½¿ç”¨pdfplumberï¼ˆæ¨èï¼Œå¯¹è¡¨æ ¼å’Œå¸ƒå±€æ”¯æŒæ›´å¥½ï¼‰
            try:
                import pdfplumber
                with pdfplumber.open(io.BytesIO(content_bytes)) as pdf:
                    text_parts = []
                    for page_num, page in enumerate(pdf.pages, 1):
                        page_text = page.extract_text()
                        if page_text:
                            text_parts.append(f"ç¬¬{page_num}é¡µ:\n{page_text}")
                    
                    if text_parts:
                        logger.info(f"PDFå†…å®¹æå–æˆåŠŸ (pdfplumber): {doc_name}, {len(text_parts)}é¡µ")
                        return "\n\n".join(text_parts)
                        
            except ImportError:
                logger.warning("pdfplumberæœªå®‰è£…ï¼Œå°è¯•pypdf")
            except Exception as e:
                logger.warning(f"pdfplumberæå–å¤±è´¥: {e}ï¼Œå°è¯•pypdf")
            
            # å›é€€åˆ°pypdf
            try:
                import pypdf
                pdf_reader = pypdf.PdfReader(io.BytesIO(content_bytes))
                text_parts = []
                
                for page_num, page in enumerate(pdf_reader.pages, 1):
                    page_text = page.extract_text()
                    if page_text and page_text.strip():
                        text_parts.append(f"ç¬¬{page_num}é¡µ:\n{page_text}")
                
                if text_parts:
                    logger.info(f"PDFå†…å®¹æå–æˆåŠŸ (pypdf): {doc_name}, {len(text_parts)}é¡µ")
                    return "\n\n".join(text_parts)
                    
            except ImportError:
                logger.error("PDFå¤„ç†åº“æœªå®‰è£…ï¼Œè¯·å®‰è£…ï¼špip install pdfplumber pypdf")
            except Exception as e:
                logger.error(f"pypdfæå–å¤±è´¥: {e}")
            
            # å¦‚æœéƒ½å¤±è´¥äº†ï¼Œè¿”å›æç¤ºä¿¡æ¯
            return f"âš ï¸ PDFæ–‡ä»¶è§£æå¤±è´¥ ({doc_name})\n\nè¿™å¯èƒ½æ˜¯å› ä¸ºï¼š\n1. PDFæ–‡ä»¶æ˜¯æ‰«æç‰ˆï¼ˆå›¾ç‰‡ï¼‰ï¼Œéœ€è¦OCRè¯†åˆ«\n2. PDFæ–‡ä»¶æœ‰å¯†ç ä¿æŠ¤\n3. PDFæ–‡ä»¶æ ¼å¼ä¸å…¼å®¹\n\nå»ºè®®ï¼šåœ¨RAGFlow Webç•Œé¢æŸ¥çœ‹è§£æåçš„åˆ†å—å†…å®¹"
                
        except Exception as e:
            logger.error(f"PDFå†…å®¹æå–å¼‚å¸¸: {e}")
            return f"âŒ PDFæ–‡ä»¶å¤„ç†å¼‚å¸¸: {str(e)}"

    def _extract_text_content(self, content_bytes: bytes) -> str:
        """
        ä»æ–‡æœ¬æ–‡ä»¶å­—èŠ‚å†…å®¹æå–æ–‡æœ¬
        
        Args:
            content_bytes: æ–‡ä»¶çš„äºŒè¿›åˆ¶å†…å®¹
            
        Returns:
            æå–çš„æ–‡æœ¬å†…å®¹
        """
        try:
            # å°è¯•æ£€æµ‹ç¼–ç 
            import chardet
            detected = chardet.detect(content_bytes)
            encoding = detected.get('encoding', 'utf-8')
            confidence = detected.get('confidence', 0)
            
            logger.debug(f"æ£€æµ‹åˆ°ç¼–ç : {encoding} (ç½®ä¿¡åº¦: {confidence:.2f})")
            
            # æŒ‰ä¼˜å…ˆçº§å°è¯•ä¸åŒç¼–ç 
            encodings_to_try = [
                encoding,  # æ£€æµ‹åˆ°çš„ç¼–ç 
                'utf-8',
                'utf-8-sig',  # UTF-8 with BOM
                'gbk',
                'gb2312', 
                'big5',
                'iso-8859-1',
                'cp1252'
            ]
            
            for enc in encodings_to_try:
                if not enc:
                    continue
                try:
                    text = content_bytes.decode(enc)
                    logger.info(f"æ–‡æœ¬å†…å®¹æå–æˆåŠŸï¼Œä½¿ç”¨ç¼–ç : {enc}")
                    return text
                except (UnicodeDecodeError, LookupError):
                    continue
            
            # æœ€åå°è¯•å¿½ç•¥é”™è¯¯
            text = content_bytes.decode('utf-8', errors='ignore')
            logger.warning("ä½¿ç”¨UTF-8ç¼–ç å¿½ç•¥é”™è¯¯æ¨¡å¼")
            return text
                
        except Exception as e:
            logger.error(f"æ–‡æœ¬å†…å®¹æå–å¼‚å¸¸: {e}")
            return f"âŒ æ–‡ä»¶å†…å®¹æå–å¤±è´¥: {str(e)}"

    def download_document(self, doc_id: str, kb_name: Optional[str] = None) -> Optional[bytes]:
        """
        ä¸‹è½½æ–‡æ¡£çš„åŸå§‹äºŒè¿›åˆ¶æ•°æ®ï¼ˆç”¨äºPDFé¢„è§ˆç­‰ï¼‰

        Args:
            doc_id: æ–‡æ¡£ID
            kb_name: çŸ¥è¯†åº“åç§°

        Returns:
            æ–‡æ¡£çš„äºŒè¿›åˆ¶æ•°æ®ï¼Œå¦‚æœå¤±è´¥è¿”å›None
        """
        try:
            kb_name = kb_name or config.default_kb_name
            dataset = self._get_or_create_dataset(kb_name)
            if not dataset:
                logger.warning(f"çŸ¥è¯†åº“ '{kb_name}' ä¸å­˜åœ¨")
                return None

            # è·å–æ–‡æ¡£ä¿¡æ¯
            docs = dataset.list_documents(id=doc_id)
            if not docs:
                logger.warning(f"æ–‡æ¡£ '{doc_id}' ä¸å­˜åœ¨")
                return None

            doc = docs[0]
            
            # ä½¿ç”¨SDKä¸‹è½½æ–‡æ¡£
            try:
                # å°è¯•è·å–æ–‡æ¡£çš„äºŒè¿›åˆ¶æ•°æ®
                binary_data = doc.download()
                if binary_data:
                    logger.info(f"æˆåŠŸä¸‹è½½æ–‡æ¡£ {doc_id}ï¼Œå¤§å°: {len(binary_data)} å­—èŠ‚")
                    return binary_data
            except Exception as e:
                logger.warning(f"SDKä¸‹è½½å¤±è´¥: {e}")
            
            # å¦‚æœSDKæ–¹æ³•å¤±è´¥ï¼Œå°è¯•å…¶ä»–æ–¹æ³•
            logger.warning(f"æ— æ³•ä¸‹è½½æ–‡æ¡£ {doc_id} çš„åŸå§‹äºŒè¿›åˆ¶æ•°æ®")
            return None

        except Exception as e:
            logger.error(f"ä¸‹è½½æ–‡æ¡£å¼‚å¸¸ {doc_id}: {e}")
            return None

    def get_document_chunks(self, doc_id: str, kb_name: Optional[str] = None) -> List[Dict[str, Any]]:
        """
        è·å–æ–‡æ¡£çš„åˆ†å—ä¿¡æ¯

        Args:
            doc_id: æ–‡æ¡£ID
            kb_name: çŸ¥è¯†åº“åç§°

        Returns:
            åˆ†å—ä¿¡æ¯åˆ—è¡¨
        """
        try:
            kb_name = kb_name or config.default_kb_name
            dataset = self._get_or_create_dataset(kb_name)
            if not dataset:
                return []

            docs = dataset.list_documents(id=doc_id)
            if not docs:
                return []

            doc = docs[0]
            chunks = doc.list_chunks()

            # è½¬æ¢ä¸ºé¢„æœŸæ ¼å¼
            chunk_list = []
            for chunk in chunks:
                chunk_list.append({
                    'id': chunk.id,
                    'content': chunk.content,
                    'important_keywords': getattr(chunk, 'important_keywords', []),
                    'available': getattr(chunk, 'available', True)
                })

            logger.info(f"Retrieved {len(chunk_list)} chunks for document {doc_id}")
            return chunk_list
        except Exception as e:
            logger.error(f"è·å–æ–‡æ¡£åˆ†å—å¤±è´¥ (doc_id: {doc_id}): {e}")
            return []

    def configure_knowledge_base(self, kb_name: Optional[str] = None) -> bool:
        """æ‰‹åŠ¨é…ç½®çŸ¥è¯†åº“

        Args:
            kb_name: çŸ¥è¯†åº“åç§°ï¼Œé»˜è®¤ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„å€¼

        Returns:
            é…ç½®æ˜¯å¦æˆåŠŸ
        """
        try:
            from src.config import get_config

            config = get_config()

            if kb_name is None:
                kb_name = config.default_kb_name

            # è·å–é…ç½®å‚æ•°
            kb_config = config.ragflow_document_config
            advanced_config = config.ragflow_advanced_config
            full_config = {**kb_config, **advanced_config}

            logger.info(f"æ‰‹åŠ¨é…ç½®çŸ¥è¯†åº“: {kb_name}")

            return self._update_knowledge_base_config(kb_name, full_config)

        except Exception as e:
            logger.error(f"æ‰‹åŠ¨é…ç½®çŸ¥è¯†åº“å¤±è´¥: {e}")
            return False

    def get_knowledge_base_config(self, kb_name: Optional[str] = None) -> dict:
        """è·å–çŸ¥è¯†åº“å½“å‰é…ç½®

        Args:
            kb_name: çŸ¥è¯†åº“åç§°

        Returns:
            çŸ¥è¯†åº“é…ç½®å­—å…¸
        """
        try:
            if kb_name is None:
                from src.config import get_config
                config = get_config()
                kb_name = config.default_kb_name

            # è·å–æ•°æ®é›†å¯¹è±¡
            dataset = self._get_or_create_dataset(kb_name)
            if not dataset:
                return {}

            # ä»æ•°æ®é›†å¯¹è±¡æå–é…ç½®
            config_info = {
                "çŸ¥è¯†åº“åŸºæœ¬ä¿¡æ¯": {
                    "åç§°": dataset.name,
                    "ID": dataset.id,
                    "åˆ†å—æ–¹æ³•": getattr(dataset, 'chunk_method', ''),
                    "åµŒå…¥æ¨¡å‹": getattr(dataset, 'embedding_model', ''),
                    "æ–‡æ¡£æ•°é‡": getattr(dataset, 'document_count', 0),
                    "åˆ†å—æ•°é‡": getattr(dataset, 'chunk_count', 0),
                    "ç›¸ä¼¼åº¦é˜ˆå€¼": getattr(dataset, 'similarity_threshold', None),
                    "å‘é‡æƒé‡": getattr(dataset, 'vector_similarity_weight', None)
                }
            }

            # å°è¯•æå–parser_config
            parser_config = getattr(dataset, 'parser_config', None)
            if parser_config:
                config_info["è§£æå™¨é…ç½®"] = parser_config

            return config_info

        except Exception as e:
            logger.warning(f"è·å–çŸ¥è¯†åº“é…ç½®å¼‚å¸¸: {e}")
            return {}

    def __enter__(self):
        """ä¸Šä¸‹æ–‡ç®¡ç†å™¨å…¥å£"""
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        """ä¸Šä¸‹æ–‡ç®¡ç†å™¨å‡ºå£"""
        # SDKä¸éœ€è¦æ˜¾å¼å…³é—­
        pass


# å…¨å±€å®¢æˆ·ç«¯å®ä¾‹
_ragflow_client: Optional[RAGFlowClient] = None


def get_ragflow_client() -> RAGFlowClient:
    """è·å–å…¨å±€RAGFlowå®¢æˆ·ç«¯å®ä¾‹"""
    global _ragflow_client
    if _ragflow_client is None:
        _ragflow_client = RAGFlowClient()
    return _ragflow_client
