#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
è°ƒè¯•RAGFlowé…ç½®æ›´æ–°
"""

import sys
import os
import json
sys.path.append(os.path.join(os.path.dirname(__file__), 'src'))

from services.ragflow_client import RAGFlowClient
import requests
import time

def debug_config_update():
    print("ğŸ”§ RAGFlowé…ç½®æ›´æ–°è°ƒè¯•")
    print("=" * 50)
    
    # åˆå§‹åŒ–å®¢æˆ·ç«¯
    client = RAGFlowClient()
    
    # è·å–çŸ¥è¯†åº“ä¿¡æ¯
    kb_id = client._get_knowledge_base_id("policy_demo_kb")
    print(f"çŸ¥è¯†åº“ID: {kb_id}")
    
    if not kb_id:
        print("âŒ æ— æ³•è·å–çŸ¥è¯†åº“ID")
        return
    
    # 1. è·å–å½“å‰é…ç½®ï¼ˆå®Œæ•´ä¿¡æ¯ï¼‰
    print("\nğŸ“‹ è·å–è¯¦ç»†é…ç½®ä¿¡æ¯...")
    
    # æ„å»ºè¯·æ±‚å‚æ•°
    base_url = "http://117.21.184.150:9380"
    headers = {
        "Authorization": "Bearer ragflow-231x38fUEnq_MSZwhOaD-6_spHL97oNJC8Wch61h0lo",
        "Content-Type": "application/json"
    }
    
    response = requests.get(
        f"{base_url}/api/v1/datasets/{kb_id}",
        headers=headers
    )
    
    if response.status_code == 200:
        data = response.json()
        if data.get("code") == 200:
            dataset_info = data.get("data", {})
            print("ğŸ“„ å®Œæ•´æ•°æ®é›†ä¿¡æ¯:")
            print(json.dumps(dataset_info, indent=2, ensure_ascii=False))
            
            # æå–å…³é”®é…ç½®ä¿¡æ¯
            chunk_method = dataset_info.get("chunk_method")
            parser_config = dataset_info.get("parser_config", {})
            
            print(f"\nğŸ” å½“å‰é…ç½®è§£æ:")
            print(f"  åˆ†å—æ–¹æ³•: {chunk_method}")
            print(f"  parser_config: {json.dumps(parser_config, indent=2, ensure_ascii=False)}")
    
    # 2. æ„å»ºæ­£ç¡®çš„æ›´æ–°è½½è·
    print("\nğŸ”„ æ„å»ºæ›´æ–°è½½è·...")
    
    # ç®€å•çš„æ›´æ–°ï¼Œåªæ”¹parser_configä¸­çš„chunk_token_num
    update_payload = {
        "parser_config": {
            "chunk_token_num": 1000  # æ”¹ä¸º1000ï¼Œå®¹æ˜“è¯†åˆ«
        }
    }
    
    print(f"ğŸ“¦ æ›´æ–°è½½è·: {json.dumps(update_payload, indent=2, ensure_ascii=False)}")
    
    # 3. å‘é€æ›´æ–°è¯·æ±‚
    print("\nğŸ“¨ å‘é€æ›´æ–°è¯·æ±‚...")
    response = requests.put(
        f"{base_url}/api/v1/datasets/{kb_id}",
        headers=headers,
        json=update_payload
    )
    
    print(f"çŠ¶æ€ç : {response.status_code}")
    print(f"å“åº”: {response.json()}")
    
    # 4. ç­‰å¾…å¹¶æ£€æŸ¥æ›´æ–°
    if response.status_code == 200 and response.json().get("code") == 200:
        print("\nâ³ ç­‰å¾…5ç§’è®©æ›´æ–°ç”Ÿæ•ˆ...")
        time.sleep(5)
        
        print("\nğŸ“‹ æ£€æŸ¥æ›´æ–°ç»“æœ...")
        check_response = requests.get(
            f"{base_url}/api/v1/datasets/{kb_id}",
            headers=headers
        )
        
        if check_response.status_code == 200:
            check_data = check_response.json()
            if check_data.get("code") == 200:
                updated_dataset = check_data.get("data", {})
                updated_parser_config = updated_dataset.get("parser_config", {})
                
                print(f"ğŸ” æ›´æ–°åçš„parser_config:")
                print(json.dumps(updated_parser_config, indent=2, ensure_ascii=False))
                
                # æ£€æŸ¥chunk_token_num
                chunk_token_num = updated_parser_config.get("chunk_token_num")
                print(f"\nâœ… chunk_token_num: {chunk_token_num}")
                
                if chunk_token_num == 1000:
                    print("ğŸ‰ é…ç½®æ›´æ–°æˆåŠŸï¼")
                else:
                    print("âŒ é…ç½®æ›´æ–°å¤±è´¥")

if __name__ == "__main__":
    debug_config_update()